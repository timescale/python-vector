[
  {
    "objectID": "vector.html",
    "href": "vector.html",
    "title": "Client",
    "section": "",
    "text": "_ = load_dotenv(find_dotenv(), override=True)\nservice_url = os.environ['TIMESCALE_SERVICE_URL']\nsource",
    "crumbs": [
      "Client"
    ]
  },
  {
    "objectID": "vector.html#usage-example",
    "href": "vector.html#usage-example",
    "title": "Client",
    "section": "Usage Example",
    "text": "Usage Example\n\nfor schema in [\"tschema\", None]:\n    vec = Async(service_url, \"data_table\", 2, schema_name=schema)\n    await vec.create_tables()\n    empty = await vec.table_is_empty()\n    assert empty\n    await vec.upsert([(uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n    empty = await vec.table_is_empty()\n    assert not empty\n\n    await vec.upsert([\n        (uuid.uuid4(), '''{\"key\":\"val\"}''', \"the brown fox\", [1.0, 1.3]),\n        (uuid.uuid4(), '''{\"key\":\"val2\", \"key_10\": \"10\", \"key_11\": \"11.3\"}''', \"the brown fox\", [1.0, 1.4]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.5]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n        (uuid.uuid4(), '''{\"key2\":\"val2\"}''', \"the brown fox\", [1.0, 1.7]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.9]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 100.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 101.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key_1\":\"val_1\", \"key_2\":\"val_2\"}''',\n        \"the brown fox\", [1.0, 1.8]),\n\n        (uuid.uuid4(), '''{\"key0\": [1,2,3,4]}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key0\": [8,9,\"A\"]}''', \"the brown fox\", [1.0, 1.8]), # mixed types\n        (uuid.uuid4(), '''{\"key0\": [5,6,7], \"key3\": 3}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key0\": [\"B\", \"C\"]}''', \"the brown fox\", [1.0, 1.8]),\n    \n    ])\n\n    await vec.create_embedding_index(IvfflatIndex())\n    await vec.drop_embedding_index()\n    await vec.create_embedding_index(IvfflatIndex(100))\n    await vec.drop_embedding_index()\n    await vec.create_embedding_index(HNSWIndex())\n    await vec.drop_embedding_index()\n    await vec.create_embedding_index(HNSWIndex(20, 125))\n    await vec.drop_embedding_index()\n    await vec.create_embedding_index(DiskAnnIndex())\n    await vec.drop_embedding_index()\n    await vec.create_embedding_index(DiskAnnIndex(50, 50, 1.5, \"memory_optimized\", 2, 1))\n\n    rec = await vec.search([1.0, 2.0])\n    assert len(rec) == 10\n    rec = await vec.search([1.0, 2.0], limit=4)\n    assert len(rec) == 4\n    rec = await vec.search(limit=4)\n    assert len(rec) == 4\n    rec = await vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"val2\"})\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"does not exist\"})\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\"})\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n    assert len(rec) == 0\n    rec = await vec.search(limit=4, filter={\"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 2\n    rec = await vec.search(limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 2\n\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}, {\"no such key\": \"no such val\"}])\n    assert len(rec) == 2\n\n    assert isinstance(rec[0][SEARCH_RESULT_METADATA_IDX], dict)\n    assert isinstance(rec[0][\"metadata\"], dict)\n    assert rec[0][\"contents\"] == \"the brown fox\"\n\n\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\")))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"==\", \"val2\")))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key\", \"==\", \"val2\"))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"&lt;\", 100))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"&lt;\", 10))\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"&lt;=\", 10))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"&lt;=\", 10.0))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_11\", \"&lt;=\", 11.3))\n    assert len(rec) == 1\n    rec = await vec.search(limit=4, predicates=Predicates(\"key_11\", \"&gt;=\", 11.29999))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_11\", \"&lt;\", 11.299999))\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [1, 2]))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [3, 7]))\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [42]))\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [4]))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [9, \"A\"]))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [\"A\"]))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", (\"C\", \"B\")))\n    assert len(rec) == 1\n\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*[(\"key\", \"val2\"), (\"key_10\", \"&lt;\", 100)]))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\"), (\"key_10\", \"&lt;\", 100), operator='AND'))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\"), (\"key_2\", \"val_2\"), operator='OR'))\n    assert len(rec) == 2\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"&lt;\", 100) & (Predicates(\"key\",\"==\", \"val2\",) | Predicates(\"key_2\", \"==\", \"val_2\"))) \n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"&lt;\", 100) and (Predicates(\"key\",\"==\", \"val2\") or Predicates(\"key_2\",\"==\", \"val_2\"))) \n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [6,7]) and Predicates(\"key3\",\"==\", 3))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key0\", \"@&gt;\", [6,7]) and Predicates(\"key3\",\"==\", 6))\n    assert len(rec) == 0\n    rec = await vec.search(limit=4, predicates=~Predicates((\"key\", \"val2\"), (\"key_10\", \"&lt;\", 100)))\n    assert len(rec) == 4\n\n    raised = False\n    try:\n        # can't upsert using both keys and dictionaries\n        await vec.upsert([\n            (uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n            (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2])\n        ])\n    except ValueError as e:\n        raised = True\n    assert raised\n\n    raised = False\n    try:\n        # can't upsert using both keys and dictionaries opposite order\n        await vec.upsert([\n            (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2]),\n            (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n        ])\n    except BaseException as e:\n        raised = True\n    assert raised\n\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 2\n    await vec.delete_by_ids([rec[0][\"id\"]])\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 1\n    await vec.delete_by_metadata([{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n    assert len(rec) == 4\n    await vec.delete_by_metadata([{\"key2\": \"val\"}])\n    rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n    assert len(rec) == 0\n\n    assert not await vec.table_is_empty()\n    await vec.delete_all()\n    assert await vec.table_is_empty()\n\n    await vec.drop_table()\n    await vec.close()\n\n    vec = Async(service_url, \"data_table\", 2, id_type=\"TEXT\")\n    await vec.create_tables()\n    empty = await vec.table_is_empty()\n    assert empty\n    await vec.upsert([(\"Not a valid UUID\", {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n    empty = await vec.table_is_empty()\n    assert not empty\n    await vec.delete_by_ids([\"Not a valid UUID\"])\n    empty = await vec.table_is_empty()\n    assert empty\n    await vec.drop_table()\n    await vec.close()\n\n    vec = Async(service_url, \"data_table\", 2, time_partition_interval=timedelta(seconds=60))\n    await vec.create_tables()\n    empty = await vec.table_is_empty()\n    assert empty\n    id = uuid.uuid1()\n    await vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n    empty = await vec.table_is_empty()\n    assert not empty\n    await vec.delete_by_ids([id])\n    empty = await vec.table_is_empty()\n    assert empty\n\n    raised = False\n    try:\n        # can't upsert with uuid type 4 in time partitioned table\n        await vec.upsert([\n            (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n        ])\n    except BaseException as e:\n        raised = True\n    assert raised\n\n    specific_datetime = datetime(2018, 8, 10, 15, 30, 0)\n    await vec.upsert([\n            # current time\n            (uuid.uuid1(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n            #time in 2018\n            (uuid_from_time(specific_datetime),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n    ])\n    assert not await vec.table_is_empty()\n\n    #check all the possible ways to specify a date range\n    async def search_date(start_date, end_date, expected):\n        #using uuid_time_filter\n        rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date, end_date))\n        assert len(rec) == expected\n        rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(str(start_date), str(end_date)))\n        assert len(rec) == expected\n    \n        #using filters\n        filter = {}\n        if start_date is not None:\n            filter[\"__start_date\"] = start_date\n        if end_date is not None:\n            filter[\"__end_date\"] = end_date\n        rec = await vec.search([1.0, 2.0], limit=4, filter=filter)\n        assert len(rec) == expected\n        #using filters with string dates\n        filter = {}\n        if start_date is not None:\n            filter[\"__start_date\"] = str(start_date)\n        if end_date is not None:\n            filter[\"__end_date\"] = str(end_date)\n        rec = await vec.search([1.0, 2.0], limit=4, filter=filter)\n        assert len(rec) == expected\n        #using predicates\n        predicates = []\n        if start_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&gt;=\", start_date))\n        if end_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&lt;\", end_date))\n        rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n        assert len(rec) == expected\n        #using predicates with string dates\n        predicates = []\n        if start_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&gt;=\", str(start_date)))\n        if end_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&lt;\", str(end_date)))\n        rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n        assert len(rec) == expected\n\n    await search_date(specific_datetime-timedelta(days=7), specific_datetime+timedelta(days=7), 1)\n    await search_date(specific_datetime-timedelta(days=7), None, 2)\n    await search_date(None, specific_datetime+timedelta(days=7), 1)\n    await search_date(specific_datetime-timedelta(days=7), specific_datetime-timedelta(days=2), 0)\n\n    #check timedelta handling\n    rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date=specific_datetime, time_delta=timedelta(days=7)))\n    assert len(rec) == 1\n    #end is exclusive\n    rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime, time_delta=timedelta(days=7)))\n    assert len(rec) == 0\n    rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime+timedelta(seconds=1), time_delta=timedelta(days=7)))\n    assert len(rec) == 1\n    rec = await vec.search([1.0, 2.0], limit=4, query_params=DiskAnnIndexParams(10, 5))\n    assert len(rec) == 2\n    rec = await vec.search([1.0, 2.0], limit=4, query_params=DiskAnnIndexParams(100))\n    assert len(rec) == 2\n    await vec.drop_table()\n    await vec.close()",
    "crumbs": [
      "Client"
    ]
  },
  {
    "objectID": "vector.html#usage-example-1",
    "href": "vector.html#usage-example-1",
    "title": "Client",
    "section": "Usage Example:",
    "text": "Usage Example:\n\nfor schema in [None, \"tschema\"]:  \n    vec = Sync(service_url, \"data_table\", 2, schema_name=schema)\n    vec.create_tables()\n    empty = vec.table_is_empty()\n\n    assert empty\n    vec.upsert([(uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n    empty = vec.table_is_empty()\n    assert not empty\n\n    vec.upsert([\n        (uuid.uuid4(), '''{\"key\":\"val\"}''', \"the brown fox\", [1.0, 1.3]),\n        (uuid.uuid4(), '''{\"key\":\"val2\"}''', \"the brown fox\", [1.0, 1.4]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.5]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n        (uuid.uuid4(), '''{\"key2\":\"val2\"}''', \"the brown fox\", [1.0, 1.7]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.9]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 100.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 101.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key_1\":\"val_1\", \"key_2\":\"val_2\"}''',\n        \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key0\": [1,2,3,4]}''', \"the brown fox\", [1.0, 1.8]),\n        (uuid.uuid4(), '''{\"key0\": [5,6,7], \"key3\": 3}''', \"the brown fox\", [1.0, 1.8]),\n    ])\n\n    vec.create_embedding_index(IvfflatIndex())\n    vec.drop_embedding_index()\n    vec.create_embedding_index(IvfflatIndex(100))\n    vec.drop_embedding_index()\n    vec.create_embedding_index(HNSWIndex())\n    vec.drop_embedding_index()\n    vec.create_embedding_index(HNSWIndex(20, 125))\n    vec.drop_embedding_index()\n    vec.create_embedding_index(DiskAnnIndex())\n    vec.drop_embedding_index()\n    vec.create_embedding_index(DiskAnnIndex(50, 50, 1.5))\n\n    rec = vec.search([1.0, 2.0])\n    assert len(rec) == 10\n    rec = vec.search(np.array([1.0, 2.0]))\n    assert len(rec) == 10\n    rec = vec.search([1.0, 2.0], limit=4)\n    assert len(rec) == 4\n    rec = vec.search(limit=4)\n    assert len(rec) == 4\n    rec = vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"val2\"})\n    assert len(rec) == 1\n    rec = vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"does not exist\"})\n    assert len(rec) == 0\n    rec = vec.search(limit=4, filter={\"key2\": \"does not exist\"})\n    assert len(rec) == 0\n    rec = vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\"})\n    assert len(rec) == 1\n    rec = vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n    assert len(rec) == 1\n    rec = vec.search([1.0, 2.0], limit=4, filter={\n                    \"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n    assert len(rec) == 0\n\n    rec = vec.search([1.0, 2.0], limit=4, filter=[\n                    {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 2\n\n    rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\n                    \"key2\": \"val2\"}, {\"no such key\": \"no such val\"}])\n    assert len(rec) == 2\n\n    raised = False\n    try:\n        # can't upsert using both keys and dictionaries\n        await vec.upsert([\n            (uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n            (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2])\n        ])\n    except ValueError as e:\n        raised = True\n    assert raised\n\n    raised = False\n    try:\n        # can't upsert using both keys and dictionaries opposite order\n        await vec.upsert([\n            (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2]),\n            (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n        ])\n    except BaseException as e:\n        raised = True\n    assert raised\n\n    rec = vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n    assert rec[0][SEARCH_RESULT_CONTENTS_IDX] == 'the brown fox'\n    assert rec[0][\"contents\"] == 'the brown fox'\n    assert rec[0][SEARCH_RESULT_METADATA_IDX] == {\n        'key_1': 'val_1', 'key_2': 'val_2'}\n    assert rec[0][\"metadata\"] == {\n        'key_1': 'val_1', 'key_2': 'val_2'}\n    assert isinstance(rec[0][SEARCH_RESULT_METADATA_IDX], dict)\n    assert rec[0][SEARCH_RESULT_DISTANCE_IDX] == 0.0009438353921149556\n    assert rec[0][\"distance\"] == 0.0009438353921149556\n\n    rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key\",\"==\", \"val2\"))\n    assert len(rec) == 1\n\n    rec = vec.search([1.0, 2.0], limit=4, filter=[\n                    {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    len(rec) == 2\n    vec.delete_by_ids([rec[0][SEARCH_RESULT_ID_IDX]])\n    rec = vec.search([1.0, 2.0], limit=4, filter=[\n                    {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 1\n    vec.delete_by_metadata([{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    rec = vec.search([1.0, 2.0], limit=4, filter=[\n                    {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n    assert len(rec) == 0\n    rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n    assert len(rec) == 4\n    vec.delete_by_metadata([{\"key2\": \"val\"}])\n    rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n    len(rec) == 0\n\n    assert not vec.table_is_empty()\n    vec.delete_all()\n    assert vec.table_is_empty()\n\n    vec.drop_table()\n    vec.close()\n\n    vec = Sync(service_url, \"data_table\", 2, id_type=\"TEXT\", schema_name=schema)\n    vec.create_tables()\n    assert vec.table_is_empty()\n    vec.upsert([(\"Not a valid UUID\", {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n    assert not vec.table_is_empty()\n    vec.delete_by_ids([\"Not a valid UUID\"])\n    assert vec.table_is_empty()\n    vec.drop_table()\n    vec.close()\n\n    vec = Sync(service_url, \"data_table\", 2, time_partition_interval=timedelta(seconds=60), schema_name=schema)\n    vec.create_tables()\n    assert vec.table_is_empty()\n    id = uuid.uuid1()\n    vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n    assert not vec.table_is_empty()\n    vec.delete_by_ids([id])\n    assert vec.table_is_empty()\n    raised = False\n    try:\n        # can't upsert with uuid type 4 in time partitioned table\n        vec.upsert([\n            (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n        ])\n        #pass\n    except BaseException as e:\n        raised = True\n    assert raised\n\n    specific_datetime = datetime(2018, 8, 10, 15, 30, 0)\n    vec.upsert([\n            # current time\n            (uuid.uuid1(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n            #time in 2018\n            (uuid_from_time(specific_datetime),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n    ])\n\n    def search_date(start_date, end_date, expected):\n        #using uuid_time_filter\n        rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date, end_date))\n        assert len(rec) == expected\n        rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(str(start_date), str(end_date)))\n        assert len(rec) == expected\n        \n        #using filters\n        filter = {}\n        if start_date is not None:\n            filter[\"__start_date\"] = start_date\n        if end_date is not None:\n            filter[\"__end_date\"] = end_date\n        rec = vec.search([1.0, 2.0], limit=4, filter=filter)\n        assert len(rec) == expected\n        #using filters with string dates\n        filter = {}\n        if start_date is not None:\n            filter[\"__start_date\"] = str(start_date)\n        if end_date is not None:\n            filter[\"__end_date\"] = str(end_date)\n        rec = vec.search([1.0, 2.0], limit=4, filter=filter)\n        assert len(rec) == expected\n        #using predicates\n        predicates = []\n        if start_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&gt;=\", start_date))\n        if end_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&lt;\", end_date))\n        rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n        assert len(rec) == expected\n        #using predicates with string dates\n        predicates = []\n        if start_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&gt;=\", str(start_date)))\n        if end_date is not None:\n            predicates.append((\"__uuid_timestamp\", \"&lt;\", str(end_date)))\n        rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n        assert len(rec) == expected\n\n    assert not vec.table_is_empty()\n\n    search_date(specific_datetime-timedelta(days=7), specific_datetime+timedelta(days=7), 1)\n    search_date(specific_datetime-timedelta(days=7), None, 2)\n    search_date(None, specific_datetime+timedelta(days=7), 1)\n    search_date(specific_datetime-timedelta(days=7), specific_datetime-timedelta(days=2), 0)\n\n    #check timedelta handling\n    rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date=specific_datetime, time_delta=timedelta(days=7)))\n    assert len(rec) == 1\n    #end is exclusive\n    rec =  vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime, time_delta=timedelta(days=7)))\n    assert len(rec) == 0\n    rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime+timedelta(seconds=1), time_delta=timedelta(days=7)))\n    assert len(rec) == 1\n    rec = vec.search([1.0, 2.0], limit=4, query_params=DiskAnnIndexParams(10, 5))\n    assert len(rec) == 2\n    rec = vec.search([1.0, 2.0], limit=4, query_params=DiskAnnIndexParams(100, rescore=2))\n    assert len(rec) == 2\n    vec.drop_table()\n    vec.close()",
    "crumbs": [
      "Client"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html",
    "href": "tsv_python_getting_started_tutorial.html",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "",
    "text": "This notebook shows how to use the PostgreSQL as vector database via the Python Vector python client library. You’ll learn how to use the client for (1) semantic search, (2) time-based vector search, (3) and how to create indexes to speed up queries.\nFollow along by downloading the Jupyter notebook version of this tutorial here.\nSample dataset: We’ll analyze a gitlog dataset (git commit messages) and use the vector embeddings of the commit messages to find relevant commit messages to a given query question. Each git commit entry has a timestamp associated with it, as well as natural language message and other metadata (e.g author, commit hash etc).",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#what-is-timescale-vector",
    "href": "tsv_python_getting_started_tutorial.html#what-is-timescale-vector",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "What is Timescale Vector?",
    "text": "What is Timescale Vector?\nTimescale Vector is PostgreSQL++ for AI applications.\nTimescale Vector enables you to efficiently store and query millions of vector embeddings in PostgreSQL.\n\nEnhances pgvector with faster and more accurate similarity search on 100M+ vectors via DiskANN inspired indexing algorithm.\nEnables fast time-based vector search via automatic time-based partitioning and indexing.\nProvides a familiar SQL interface for querying vector embeddings and relational data.\n\nTimescale Vector is cloud PostgreSQL for AI that scales with you from POC to production:\n\nSimplifies operations by enabling you to store relational metadata, vector embeddings, and time-series data in a single database.\nBenefits from rock-solid PostgreSQL foundation with enterprise-grade feature liked streaming backups and replication, high-availability and row-level security.\nEnables a worry-free experience with enterprise-grade security and compliance.",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#how-to-access-timescale-vector",
    "href": "tsv_python_getting_started_tutorial.html#how-to-access-timescale-vector",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "How to access Timescale Vector",
    "text": "How to access Timescale Vector\nTimescale Vector is available on Timescale, the cloud PostgreSQL platform. (There is no self-hosted version at this time.)\nGood news! You get a 90-day free trial for Timescale Vector:\n\nTo get started, signup to Timescale, create a new database and follow this notebook!\nSee the Timescale Vector explainer blog for more details and performance benchmarks.",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#setup",
    "href": "tsv_python_getting_started_tutorial.html#setup",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "0. Setup",
    "text": "0. Setup\nDownload the Jupyter notebook version of this tutorial.\n\n# install needed packages\n!pip install timescale-vector\n!pip install openai\n!pip install tiktoken\n!pip install python-dotenv\n\n\n# import needed packages\nimport os\nfrom dotenv import load_dotenv, find_dotenv\nimport timescale_vector\nfrom timescale_vector import client\nimport openai\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport json\nimport tiktoken\nimport ast\nimport math\nimport uuid\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom typing import List, Tuple\n\nWe’ll use OpenAI’s embedding models so let’s load our OpenAI API keys from a .env file.\nIf you do not have an OpenAI API Key, signup for an OpenAI Developer Account and create an API Key. See OpenAI’s developer platform for more information.\n\n# Run export OPENAI_API_KEY=sk-YOUR_OPENAI_API_KEY...\n# Get openAI api key by reading local .env file\n\n_ = load_dotenv(find_dotenv())\nopenai.api_key  = os.environ['OPENAI_API_KEY']\n\nNext let’s load our Timescale service URL which we’ll use to connect to our cloud PostgreSQL database hosted on Timescale.\nLaunch a PostgreSQL database on Timescale and download the .env file after your service is created.\n\n# Get the service url by reading local .env file\n# The .env file should contain a line starting with `TIMESCALE_SERVICE_URL=postgresql://`\n_ = load_dotenv(find_dotenv())\nTIMESCALE_SERVICE_URL = os.environ[\"TIMESCALE_SERVICE_URL\"]\n\n# OR set it explicitly\n# TIMESCALE_SERVICE_URL = \"postgres://tsdbadmin:&lt;password&gt;@&lt;id&gt;.tsdb.cloud.timescale.com:&lt;port&gt;/tsdb?sslmode=require\"",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#create-a-table-to-store-the-vectors-and-metadata",
    "href": "tsv_python_getting_started_tutorial.html#create-a-table-to-store-the-vectors-and-metadata",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "1. Create a table to store the vectors and metadata",
    "text": "1. Create a table to store the vectors and metadata\nFirst, we’ll define a table name, which will be the name of our table in the PostgreSQL database.\nWe set the time_partition_interval argument in the client creation function to enable automatic time-based partitioning of the table. This will partition the table into time-based chunks (in this case each containing data for 7 days) and create indexes on the time-based chunks to speed up time-based queries.\nEach partition will consist of data for the specified length of time. We’ll use 7 days for simplicity, but you can pick whatever value make sense for your use case – for example if you query recent vectors frequently you might want to use a smaller time delta like 1 day, or if you query vectors over a decade long time period then you might want to use a larger time delta like 6 months or 1 year.\n\n# Table information\nTABLE_NAME = \"commit_history\"\nEMBEDDING_DIMENSIONS = 1536\nTIME_PARTITION_INTERVAL = timedelta(days=7)\n\n# Create client object\nvec = client.Async(TIMESCALE_SERVICE_URL, \n                   TABLE_NAME,  \n                   EMBEDDING_DIMENSIONS, \n                   time_partition_interval=TIME_PARTITION_INTERVAL)\n\n# create table\nawait vec.create_tables()\n\nThe create_tables() function will create a table with the following schema:\nid | metadata | contents | embedding\n\nid is the UUID which uniquely identifies each vector.\nmetadata is a JSONB column which stores the metadata associated with each vector.\ncontents is the text column which stores the content we want vectorized (in this case the commit message).\nembedding is the vector column which stores the vector embedding representation of the content.",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#load-in-dataset-create-vector-embeddings-and-prepare-data-for-ingestion",
    "href": "tsv_python_getting_started_tutorial.html#load-in-dataset-create-vector-embeddings-and-prepare-data-for-ingestion",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "2. Load in dataset, create vector embeddings, and prepare data for ingestion",
    "text": "2. Load in dataset, create vector embeddings, and prepare data for ingestion\n\nLoad sample dataset\nFirst, you’ll need to download the sample dataset and place it in the same directory as this notebook.\nYou can use following command:\n\n# Download the file using curl and save it as commit_history.csv\n# Note: Execute this command in your terminal, in the same directory as the notebook\n# curl -O https://s3.amazonaws.com/assets.timescale.com/ai/commit_history.csv\n\nThen, we’ll load in the gitlog dataset CSV file into a pandas dataframe.\nNote: Since this is a demo, we will only work with the first 1000 records. In practice, you can load as many records as you want.\n\n# Set the path to the dataset file relative to this notebook\nfile_path = Path(\"commit_history.csv\")\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv(file_path)\n\n# Light data cleaning on CSV\ndf.dropna(inplace=True)\ndf = df.astype(str)\ndf = df[:1000]\n\n\n# Take a look at the data in the csv (optional)\ndf.head()\n\n\n\n\n\n\n\n\ncommit\nauthor\ndate\nchange summary\nchange details\n\n\n\n\n0\n44e41c12ab25e36c202f58e068ced262eadc8d16\nLakshmi Narayanan Sreethar&lt;lakshmi@timescale.com&gt;\nTue Sep 5 21:03:21 2023 +0530\nFix segfault in set_integer_now_func\nWhen an invalid function oid is passed to set_...\n\n\n1\ne66a40038e3c84fb1a68da67ad71caf75c64a027\nBharathy&lt;satish.8483@gmail.com&gt;\nSat Sep 2 09:24:31 2023 +0530\nFix server crash on UPDATE of compressed chunk\nUPDATE query with system attributes in WHERE c...\n\n\n2\nc6a930897e9f9e9878db031cc7fb6ea79d721a74\nJan Nidzwetzki&lt;jan@timescale.com&gt;\nTue Aug 29 21:13:51 2023 +0200\nUse Debian Bookworm for 32-bit tests\nSo far, we have used Debian Buster (10) for ou...\n\n\n3\n8e941b80ae1b0e0b6affe5431454cdc637628d99\nLakshmi Narayanan Sreethar&lt;lakshmi@timescale.com&gt;\nMon Aug 28 23:19:22 2023 +0530\nFix incorrect row count in EXPLAIN ANALYZE INS...\nINSERT ... ON CONFLICT statements record few m...\n\n\n4\ncaada43454e25d3098744fa6b675ac7d07390550\nLakshmi Narayanan Sreethar&lt;lakshmi@timescale.com&gt;\nTue May 30 20:32:29 2023 +0530\nPG16: Fix concurrent update issues with MERGE.\nPG16 commit postgres/postgres@9321c79c fixes a...\n\n\n\n\n\n\n\n\n\nCreate vector embeddings\nNext we’ll create vector embeddings of the commit messages using the OpenAI API. We’ll use the text-embedding-ada-002 model to create the embeddings (Learn more here).\n\n# Create embeddings for each commit message\n\nembedding_list = []\ncontent_list = []\n\nopenai_client = openai.Client()\n\n# Helper function: get embeddings for a text\ndef get_embeddings(text):\n    response = openai_client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input = text.replace(\"\\n\",\" \")\n    )\n    embedding = response.data[0].embedding\n    return embedding\n\nfor index, row in df.iterrows():\n    #construct text we want to embed\n    text = row['author'] + \" \"+ row['date'] + \" \" +row['commit']+ \" \" + row['change summary'] + \" \"+ row['change details']\n    content_list.append(text)\n    embedding = get_embeddings(text)\n    embedding_list.append(embedding)\n\n\n# Append embddings and content to dataframe\ndf['content'] = content_list\ndf['embedding'] = embedding_list\n\n\n\nPrepare data for ingestion\nNext, we’ll create a uuid for each git log entry.\nWe’ll define a helper funciton create_uuid() to create a uuid for commit message and associated vector embedding based on its timestamp.\nIn the helper function, we’ll use the timescale vector client library’s uuid_from_time() method to take a date and create a uuid with a datetime portion that reflects the date string.\n\n# Create uuids for each message\nuuid_list = []\n\n\n# helper function to take in a date string in the past and return a uuid v1\ndef create_uuid(date_string: str):\n    if date_string is None:\n        return None\n    time_format = '%a %b %d %H:%M:%S %Y %z'\n    datetime_obj = datetime.strptime(date_string, time_format)\n    uuid = client.uuid_from_time(datetime_obj)\n    return str(uuid)\n\nfor index, row in df.iterrows():\n    uuid = create_uuid(row['date'])\n    uuid_list.append(uuid)\n\n\n# Add uuids to dataframe\ndf['uuid'] = uuid_list\n\nFinally, let’s create a json of metadata for each entry in our dataset.\nWe’ll again use some helper functions to ensure we have data in the right format for easy filtering.\n\n# Helper functions\n# Helper function to split name and email given an author string consisting of Name Lastname &lt;email&gt;\ndef split_name(input_string: str) -&gt; Tuple[str, str]:\n    if input_string is None:\n        return None, None\n    start = input_string.find(\"&lt;\")\n    end = input_string.find(\"&gt;\")\n    name = input_string[:start].strip()\n    return name\n\ndef create_date(input_string: str) -&gt; datetime:\n    if input_string is None:\n        return None\n    # Define a dictionary to map month abbreviations to their numerical equivalents\n    month_dict = {\n        \"Jan\": \"01\",\n        \"Feb\": \"02\",\n        \"Mar\": \"03\",\n        \"Apr\": \"04\",\n        \"May\": \"05\",\n        \"Jun\": \"06\",\n        \"Jul\": \"07\",\n        \"Aug\": \"08\",\n        \"Sep\": \"09\",\n        \"Oct\": \"10\",\n        \"Nov\": \"11\",\n        \"Dec\": \"12\",\n    }\n\n    # Split the input string into its components\n    components = input_string.split()\n    # Extract relevant information\n    day = components[2]\n    month = month_dict[components[1]]\n    year = components[4]\n    time = components[3]\n    timezone_offset_minutes = int(components[5])  # Convert the offset to minutes\n    timezone_hours = timezone_offset_minutes // 60  # Calculate the hours\n    timezone_minutes = timezone_offset_minutes % 60  # Calculate the remaining minutes\n    # Create a formatted string for the timestamptz in PostgreSQL format\n    timestamp_tz_str = f\"{year}-{month}-{day} {time}+{timezone_hours:02}{timezone_minutes:02}\"\n    return timestamp_tz_str\n\n\nmetadata_list = []\n\nfor index, row in df.iterrows():\n    metadata = {\n        \"author\": split_name(row['author']),\n        \"date\": create_date(row['date']),\n        \"commit\": row['commit'],\n    }\n    metadata_list.append(metadata)\n\n# Add metadata to dataframe\ndf['metadata'] = metadata_list\n\n\nprint(metadata_list[0])\n\n{'author': 'Lakshmi Narayanan Sreethar', 'date': '2023-09-5 21:03:21+0850', 'commit': '44e41c12ab25e36c202f58e068ced262eadc8d16'}\n\n\nWe’ve finished loading in our dataset and preparing it for storage in Timescale Vector.\nNote that we’ve explained each step of the data preparation process in detail for the purposes of this tutorial. In practice, you can create embeddings, uuids and metadata in a single step.",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#add-data-to-timescale-vector",
    "href": "tsv_python_getting_started_tutorial.html#add-data-to-timescale-vector",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "3. Add data to Timescale Vector",
    "text": "3. Add data to Timescale Vector\nNext, we’ll add our commit history data to the commit_history table in Timescale Vector.\nWe’ll prepare a list of tuples to add to the table. Each tuple will contain the following data:\n(id, metadata, contents, embedding)\nto match the schema of our table.\n\n# Remind ourselves of the data in the dataframe\n# df.head()\n\n\n# convert dataframe to array of tuples\nrecords = []\nfor index, row in df.iterrows():\n    record = (row['uuid'], row['metadata'], row['content'], row['embedding'])\n    records.append(record)\n\nLastly, we batch upsert the data into the table using the .upsert() method passing in our list of tuples we prepared above.\n\n# batch upsert vectors into table\nawait vec.upsert(records)",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#similarity-search-with-timescale-vector",
    "href": "tsv_python_getting_started_tutorial.html#similarity-search-with-timescale-vector",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "4. Similarity search with Timescale Vector",
    "text": "4. Similarity search with Timescale Vector\n\nSimple similarity search\nLet’s see an example of performing a simple similarity search using Timescale Vector, where we’ll find the most similar commit messages to a given query by performing a nearest neighbor search.\n\n# define search query and query_embedding\nquery_string = \"What's new with continuous aggregates?\"\nquery_embedding = get_embeddings(query_string)\n\n\n# search table for similar vectors to query_embedding\nrecords = await vec.search(query_embedding)\n\nThe response from the .search() method returns a list Records objects. Each Record object contains the following attributes: id, metadata, contents, embedding, distance.\nThe results are sorted by distance. The first result is the most similar to our query.\nLet’s inspect them below:\n\nrecords\n\n[&lt;Record id=UUID('18331d00-fc57-11ec-a166-06cee12dbc78') metadata={'date': '2022-07-5 13:39:14+0320', 'author': 'Fabrízio de Royes Mello', 'commit': ' e34218ce2963358a500f6bc315aace0fad29c450'} contents=\"Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Tue Jul 5 13:39:14 2022 +0200  e34218ce2963358a500f6bc315aace0fad29c450 Migrate Continuous Aggregates to the new format Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  When upgrading to Timescale 2.7, new created Continuous Aggregates are using the new format, but existing Continuous Aggregates keep using the format they were defined with.  Created a procedure to upgrade existing Continuous Aggregates from the old format to the new format, by calling a simple procedure:  test=# CALL cagg_migrate('conditions_summary_daily');  Closes #4424 \" embedding=array([-0.02072006, -0.00232497, -0.00290987, ..., -0.00420762,\n        -0.00879542, -0.02118798], dtype=float32) distance=0.15402132505614874&gt;,\n &lt;Record id=UUID('18331d00-fc57-11ec-8f40-352ea14812b8') metadata={'date': '2022-07-5 13:39:14+0320', 'author': 'Fabrízio de Royes Mello', 'commit': ' e34218ce2963358a500f6bc315aace0fad29c450'} contents=\"Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Tue Jul 5 13:39:14 2022 +0200  e34218ce2963358a500f6bc315aace0fad29c450 Migrate Continuous Aggregates to the new format Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  When upgrading to Timescale 2.7, new created Continuous Aggregates are using the new format, but existing Continuous Aggregates keep using the format they were defined with.  Created a procedure to upgrade existing Continuous Aggregates from the old format to the new format, by calling a simple procedure:  test=# CALL cagg_migrate('conditions_summary_daily');  Closes #4424 \" embedding=array([-0.02072006, -0.00232497, -0.00290987, ..., -0.00420762,\n        -0.00879542, -0.02118798], dtype=float32) distance=0.15402132505614874&gt;,\n &lt;Record id=UUID('c98d1c00-6c13-11ed-b2e5-ba3746d2d4a5') metadata={'date': '2022-11-24 13:19:36+-500', 'author': 'Fabrízio de Royes Mello', 'commit': ' 35c91204987ccb0161d745af1a39b7eb91bc65a5'} contents='Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Thu Nov 24 13:19:36 2022 -0300  35c91204987ccb0161d745af1a39b7eb91bc65a5 Add Hierarchical Continuous Aggregates validations Commit 3749953e introduce Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) but it lacks of some basic validations.  Validations added during the creation of a Hierarchical Continuous Aggregate:  * Forbid create a continuous aggregate with fixed-width bucket on top of   a continuous aggregate with variable-width bucket.  * Forbid incompatible bucket widths:   - should not be equal;   - bucket width of the new continuous aggregate should be greater than     the source continuous aggregate;   - bucket width of the new continuous aggregate should be multiple of     the source continuous aggregate. ' embedding=array([-0.03262706, -0.0018098 , -0.01641467, ...,  0.00157952,\n        -0.01413165, -0.01476743], dtype=float32) distance=0.15454035563716317&gt;,\n &lt;Record id=UUID('c98d1c00-6c13-11ed-9626-aeca4bbf6c5d') metadata={'date': '2022-11-24 13:19:36+-500', 'author': 'Fabrízio de Royes Mello', 'commit': ' 35c91204987ccb0161d745af1a39b7eb91bc65a5'} contents='Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Thu Nov 24 13:19:36 2022 -0300  35c91204987ccb0161d745af1a39b7eb91bc65a5 Add Hierarchical Continuous Aggregates validations Commit 3749953e introduce Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) but it lacks of some basic validations.  Validations added during the creation of a Hierarchical Continuous Aggregate:  * Forbid create a continuous aggregate with fixed-width bucket on top of   a continuous aggregate with variable-width bucket.  * Forbid incompatible bucket widths:   - should not be equal;   - bucket width of the new continuous aggregate should be greater than     the source continuous aggregate;   - bucket width of the new continuous aggregate should be multiple of     the source continuous aggregate. ' embedding=array([-0.03262706, -0.0018098 , -0.01641467, ...,  0.00157952,\n        -0.01413165, -0.01476743], dtype=float32) distance=0.15454035563716317&gt;,\n &lt;Record id=UUID('2144db80-88bd-11ec-8cea-ce147abb2c4b') metadata={'date': '2022-02-8 09:57:23+0140', 'author': 'Erik Nordström', 'commit': ' 5af9f45488d51027804cac16362811f71a89bb64'} contents='Erik Nordström&lt;erik@timescale.com&gt; Tue Feb 8 09:57:23 2022 +0100  5af9f45488d51027804cac16362811f71a89bb64 Add extra telemetry for continuous aggregates Add the following telemetry fields for continuous aggregates:  * The number of continuous aggregates created on distributed   hypertables * The number of continuous aggregates using real-time aggregation ' embedding=array([-0.02287812, -0.02124397,  0.01628467, ..., -0.01387608,\n        -0.01325794, -0.01354214], dtype=float32) distance=0.15761212923621704&gt;,\n &lt;Record id=UUID('2144db80-88bd-11ec-a0f8-155e201a9074') metadata={'date': '2022-02-8 09:57:23+0140', 'author': 'Erik Nordström', 'commit': ' 5af9f45488d51027804cac16362811f71a89bb64'} contents='Erik Nordström&lt;erik@timescale.com&gt; Tue Feb 8 09:57:23 2022 +0100  5af9f45488d51027804cac16362811f71a89bb64 Add extra telemetry for continuous aggregates Add the following telemetry fields for continuous aggregates:  * The number of continuous aggregates created on distributed   hypertables * The number of continuous aggregates using real-time aggregation ' embedding=array([-0.02287812, -0.02124397,  0.01628467, ..., -0.01387608,\n        -0.01325794, -0.01354214], dtype=float32) distance=0.15761212923621704&gt;,\n &lt;Record id=UUID('dddb6100-d17a-11ec-8fe6-3457c1f43f23') metadata={'date': '2022-05-11 19:36:58+-500', 'author': 'Fabrízio de Royes Mello', 'commit': ' f266f5cf564fcc5509b91493a39eb201c6f5914a'} contents=\"Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed May 11 19:36:58 2022 -0300  f266f5cf564fcc5509b91493a39eb201c6f5914a Continuous Aggregates finals form Following work started by #4294 to improve performance of Continuous Aggregates by removing the re-aggregation in the user view.  This PR get rid of `partialize_agg` and `finalize_agg` aggregate functions and store the finalized aggregated (plain) data in the materialization hypertable.  Because we're not storing partials anymore and removed the re-aggregation, now is be possible to create indexes on aggregated columns in the materialization hypertable in order to improve the performance even more.  Also removed restrictions on types of aggregates users can perform with Continuous Aggregates: * aggregates with DISTINCT * aggregates with FILTER * aggregates with FILTER in HAVING clause * aggregates without combine function * ordered-set aggregates * hypothetical-set aggregates  By default new Continuous Aggregates will be created using this new format, but the previous version (with partials) will be supported.  Users can create the previous style by setting to `false` the storage paramater named `timescaledb.finalized` during the creation of the Continuous Aggregate.  Fixes #4233 \" embedding=array([-0.03077092,  0.0143465 , -0.01135488, ..., -0.00501059,\n        -0.01490651, -0.02304872], dtype=float32) distance=0.1637590571138441&gt;,\n &lt;Record id=UUID('dddb6100-d17a-11ec-825d-6556061133f2') metadata={'date': '2022-05-11 19:36:58+-500', 'author': 'Fabrízio de Royes Mello', 'commit': ' f266f5cf564fcc5509b91493a39eb201c6f5914a'} contents=\"Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed May 11 19:36:58 2022 -0300  f266f5cf564fcc5509b91493a39eb201c6f5914a Continuous Aggregates finals form Following work started by #4294 to improve performance of Continuous Aggregates by removing the re-aggregation in the user view.  This PR get rid of `partialize_agg` and `finalize_agg` aggregate functions and store the finalized aggregated (plain) data in the materialization hypertable.  Because we're not storing partials anymore and removed the re-aggregation, now is be possible to create indexes on aggregated columns in the materialization hypertable in order to improve the performance even more.  Also removed restrictions on types of aggregates users can perform with Continuous Aggregates: * aggregates with DISTINCT * aggregates with FILTER * aggregates with FILTER in HAVING clause * aggregates without combine function * ordered-set aggregates * hypothetical-set aggregates  By default new Continuous Aggregates will be created using this new format, but the previous version (with partials) will be supported.  Users can create the previous style by setting to `false` the storage paramater named `timescaledb.finalized` during the creation of the Continuous Aggregate.  Fixes #4233 \" embedding=array([-0.03077092,  0.0143465 , -0.01135488, ..., -0.00501059,\n        -0.01490651, -0.02304872], dtype=float32) distance=0.1637590571138441&gt;,\n &lt;Record id=UUID('0df31a00-44f7-11ed-82a8-18143619d1eb') metadata={'date': '2022-10-5 18:45:40+-500', 'author': 'Fabrízio de Royes Mello', 'commit': ' 3749953e9704e45df8f621607989ada0714ce28d'} contents='Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed Oct 5 18:45:40 2022 -0300  3749953e9704e45df8f621607989ada0714ce28d Hierarchical Continuous Aggregates Enable users create Hierarchical Continuous Aggregates (aka Continuous Aggregates on top of another Continuous Aggregates).  With this PR users can create levels of aggregation granularity in Continuous Aggregates making the refresh process even faster.  A problem with this feature can be in upper levels we can end up with the \"average of averages\". But to get the \"real average\" we can rely on \"stats_aggs\" TimescaleDB Toolkit function that calculate and store the partials that can be finalized with other toolkit functions like \"average\" and \"sum\".  Closes #1400 ' embedding=array([-0.02771199,  0.01358744,  0.00311197, ..., -0.00547272,\n        -0.01917629, -0.03033948], dtype=float32) distance=0.16440806282766374&gt;,\n &lt;Record id=UUID('0df31a00-44f7-11ed-99ea-57328005937d') metadata={'date': '2022-10-5 18:45:40+-500', 'author': 'Fabrízio de Royes Mello', 'commit': ' 3749953e9704e45df8f621607989ada0714ce28d'} contents='Fabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed Oct 5 18:45:40 2022 -0300  3749953e9704e45df8f621607989ada0714ce28d Hierarchical Continuous Aggregates Enable users create Hierarchical Continuous Aggregates (aka Continuous Aggregates on top of another Continuous Aggregates).  With this PR users can create levels of aggregation granularity in Continuous Aggregates making the refresh process even faster.  A problem with this feature can be in upper levels we can end up with the \"average of averages\". But to get the \"real average\" we can rely on \"stats_aggs\" TimescaleDB Toolkit function that calculate and store the partials that can be finalized with other toolkit functions like \"average\" and \"sum\".  Closes #1400 ' embedding=array([-0.02771199,  0.01358744,  0.00311197, ..., -0.00547272,\n        -0.01917629, -0.03033948], dtype=float32) distance=0.16440806282766374&gt;]\n\n\nLet’s look at how to access the fields of a record object. The client provides helper methods to easily access the fields of a record object.\nWe’ll use them inspect the content of the results to see that they are indeed relevant to the query we searched for.\n\nfor result in records:\n    print(\"-\" * 80)\n    print(result[client.SEARCH_RESULT_CONTENTS_IDX])\n\n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Tue Jul 5 13:39:14 2022 +0200  e34218ce2963358a500f6bc315aace0fad29c450 Migrate Continuous Aggregates to the new format Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  When upgrading to Timescale 2.7, new created Continuous Aggregates are using the new format, but existing Continuous Aggregates keep using the format they were defined with.  Created a procedure to upgrade existing Continuous Aggregates from the old format to the new format, by calling a simple procedure:  test=# CALL cagg_migrate('conditions_summary_daily');  Closes #4424 \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Tue Jul 5 13:39:14 2022 +0200  e34218ce2963358a500f6bc315aace0fad29c450 Migrate Continuous Aggregates to the new format Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  When upgrading to Timescale 2.7, new created Continuous Aggregates are using the new format, but existing Continuous Aggregates keep using the format they were defined with.  Created a procedure to upgrade existing Continuous Aggregates from the old format to the new format, by calling a simple procedure:  test=# CALL cagg_migrate('conditions_summary_daily');  Closes #4424 \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Thu Nov 24 13:19:36 2022 -0300  35c91204987ccb0161d745af1a39b7eb91bc65a5 Add Hierarchical Continuous Aggregates validations Commit 3749953e introduce Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) but it lacks of some basic validations.  Validations added during the creation of a Hierarchical Continuous Aggregate:  * Forbid create a continuous aggregate with fixed-width bucket on top of   a continuous aggregate with variable-width bucket.  * Forbid incompatible bucket widths:   - should not be equal;   - bucket width of the new continuous aggregate should be greater than     the source continuous aggregate;   - bucket width of the new continuous aggregate should be multiple of     the source continuous aggregate. \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Thu Nov 24 13:19:36 2022 -0300  35c91204987ccb0161d745af1a39b7eb91bc65a5 Add Hierarchical Continuous Aggregates validations Commit 3749953e introduce Hierarchical Continuous Aggregates (aka Continuous Aggregate on top of another Continuous Aggregate) but it lacks of some basic validations.  Validations added during the creation of a Hierarchical Continuous Aggregate:  * Forbid create a continuous aggregate with fixed-width bucket on top of   a continuous aggregate with variable-width bucket.  * Forbid incompatible bucket widths:   - should not be equal;   - bucket width of the new continuous aggregate should be greater than     the source continuous aggregate;   - bucket width of the new continuous aggregate should be multiple of     the source continuous aggregate. \n--------------------------------------------------------------------------------\nErik Nordström&lt;erik@timescale.com&gt; Tue Feb 8 09:57:23 2022 +0100  5af9f45488d51027804cac16362811f71a89bb64 Add extra telemetry for continuous aggregates Add the following telemetry fields for continuous aggregates:  * The number of continuous aggregates created on distributed   hypertables * The number of continuous aggregates using real-time aggregation \n--------------------------------------------------------------------------------\nErik Nordström&lt;erik@timescale.com&gt; Tue Feb 8 09:57:23 2022 +0100  5af9f45488d51027804cac16362811f71a89bb64 Add extra telemetry for continuous aggregates Add the following telemetry fields for continuous aggregates:  * The number of continuous aggregates created on distributed   hypertables * The number of continuous aggregates using real-time aggregation \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed May 11 19:36:58 2022 -0300  f266f5cf564fcc5509b91493a39eb201c6f5914a Continuous Aggregates finals form Following work started by #4294 to improve performance of Continuous Aggregates by removing the re-aggregation in the user view.  This PR get rid of `partialize_agg` and `finalize_agg` aggregate functions and store the finalized aggregated (plain) data in the materialization hypertable.  Because we're not storing partials anymore and removed the re-aggregation, now is be possible to create indexes on aggregated columns in the materialization hypertable in order to improve the performance even more.  Also removed restrictions on types of aggregates users can perform with Continuous Aggregates: * aggregates with DISTINCT * aggregates with FILTER * aggregates with FILTER in HAVING clause * aggregates without combine function * ordered-set aggregates * hypothetical-set aggregates  By default new Continuous Aggregates will be created using this new format, but the previous version (with partials) will be supported.  Users can create the previous style by setting to `false` the storage paramater named `timescaledb.finalized` during the creation of the Continuous Aggregate.  Fixes #4233 \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed May 11 19:36:58 2022 -0300  f266f5cf564fcc5509b91493a39eb201c6f5914a Continuous Aggregates finals form Following work started by #4294 to improve performance of Continuous Aggregates by removing the re-aggregation in the user view.  This PR get rid of `partialize_agg` and `finalize_agg` aggregate functions and store the finalized aggregated (plain) data in the materialization hypertable.  Because we're not storing partials anymore and removed the re-aggregation, now is be possible to create indexes on aggregated columns in the materialization hypertable in order to improve the performance even more.  Also removed restrictions on types of aggregates users can perform with Continuous Aggregates: * aggregates with DISTINCT * aggregates with FILTER * aggregates with FILTER in HAVING clause * aggregates without combine function * ordered-set aggregates * hypothetical-set aggregates  By default new Continuous Aggregates will be created using this new format, but the previous version (with partials) will be supported.  Users can create the previous style by setting to `false` the storage paramater named `timescaledb.finalized` during the creation of the Continuous Aggregate.  Fixes #4233 \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed Oct 5 18:45:40 2022 -0300  3749953e9704e45df8f621607989ada0714ce28d Hierarchical Continuous Aggregates Enable users create Hierarchical Continuous Aggregates (aka Continuous Aggregates on top of another Continuous Aggregates).  With this PR users can create levels of aggregation granularity in Continuous Aggregates making the refresh process even faster.  A problem with this feature can be in upper levels we can end up with the \"average of averages\". But to get the \"real average\" we can rely on \"stats_aggs\" TimescaleDB Toolkit function that calculate and store the partials that can be finalized with other toolkit functions like \"average\" and \"sum\".  Closes #1400 \n--------------------------------------------------------------------------------\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Wed Oct 5 18:45:40 2022 -0300  3749953e9704e45df8f621607989ada0714ce28d Hierarchical Continuous Aggregates Enable users create Hierarchical Continuous Aggregates (aka Continuous Aggregates on top of another Continuous Aggregates).  With this PR users can create levels of aggregation granularity in Continuous Aggregates making the refresh process even faster.  A problem with this feature can be in upper levels we can end up with the \"average of averages\". But to get the \"real average\" we can rely on \"stats_aggs\" TimescaleDB Toolkit function that calculate and store the partials that can be finalized with other toolkit functions like \"average\" and \"sum\".  Closes #1400 \n\n\nHere’s how to access each field of a record object:\n\nresult = records[0]\n\nrecord_id = result[client.SEARCH_RESULT_ID_IDX]\nrecord_metadata = result[client.SEARCH_RESULT_METADATA_IDX]\nrecord_embedding = result[client.SEARCH_RESULT_EMBEDDING_IDX]\nrecord_distance = result[client.SEARCH_RESULT_DISTANCE_IDX]\nrecord_contents = result[client.SEARCH_RESULT_CONTENTS_IDX]\n\n\nprint(\"-\" * 80)\nprint(record_id)\nprint(record_metadata)\nprint(record_contents)\nprint(record_distance)\nprint(\"-\" * 80)\n\n--------------------------------------------------------------------------------\n18331d00-fc57-11ec-a166-06cee12dbc78\n{'date': '2022-07-5 13:39:14+0320', 'author': 'Fabrízio de Royes Mello', 'commit': ' e34218ce2963358a500f6bc315aace0fad29c450'}\nFabrízio de Royes Mello&lt;fabriziomello@gmail.com&gt; Tue Jul 5 13:39:14 2022 +0200  e34218ce2963358a500f6bc315aace0fad29c450 Migrate Continuous Aggregates to the new format Timescale 2.7 released a new version of Continuous Aggregate (#4269) that store the final aggregation state instead of the byte array of the partial aggregate state, offering multiple opportunities of optimizations as well a more compact form.  When upgrading to Timescale 2.7, new created Continuous Aggregates are using the new format, but existing Continuous Aggregates keep using the format they were defined with.  Created a procedure to upgrade existing Continuous Aggregates from the old format to the new format, by calling a simple procedure:  test=# CALL cagg_migrate('conditions_summary_daily');  Closes #4424 \n0.15402132505614874\n--------------------------------------------------------------------------------\n\n\n\n\nSimilarity search with metadata filtering\nTimecale Vector also supports filtering results by metadata. Let’s see an example of this, where we’ll specify the number of results to return using the limit argument and filter the results by the author metadata field.\n\nrecords_filtered = await vec.search(query_embedding, limit=3, filter={\"author\": \"Rafia Sabih\"})\n\n\nfor result in records_filtered:\n    print(\"-\" * 80)\n    print(result[client.SEARCH_RESULT_CONTENTS_IDX])\n\n--------------------------------------------------------------------------------\nRafia Sabih&lt;rafia.sabih@gmail.com&gt; Wed Feb 8 11:54:28 2023 +0100  98218c1d079231a9aa469b37ddd0ed39e77c2adb Enable joins for heirarchical continuous aggregates The joins could be between a continuous aggregate and hypertable, continuous aggregate and a regular Postgres table, and continuous aggregate and a regular Postgres view. \n--------------------------------------------------------------------------------\nRafia Sabih&lt;rafia.sabih@gmail.com&gt; Wed Feb 8 11:54:28 2023 +0100  98218c1d079231a9aa469b37ddd0ed39e77c2adb Enable joins for heirarchical continuous aggregates The joins could be between a continuous aggregate and hypertable, continuous aggregate and a regular Postgres table, and continuous aggregate and a regular Postgres view. \n--------------------------------------------------------------------------------\nRafia Sabih&lt;rafia.sabih@gmail.com&gt; Thu Apr 27 15:01:38 2023 +0200  d9849325d0d0f81a13db1e41aa56f8b567945e72 Improve test suite Add more regression tests for Continuous aggregates with joins. \n\n\nWe can from the output above that we only get results back from the author we filtered for!",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#using-ann-search-indexes-to-speed-up-queries",
    "href": "tsv_python_getting_started_tutorial.html#using-ann-search-indexes-to-speed-up-queries",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "5. Using ANN search indexes to speed up queries",
    "text": "5. Using ANN search indexes to speed up queries\nYou can speed up similarity queries by creating an index on the embedding column. We recommend doing this if you have large number (10k+) vectors in your table.\nTimescale Vector supports the following indexes: - timescale_vector_index: a disk-ann inspired graph index for fast similarity search (default). - pgvector’s HNSW index: a hierarchical navigable small world graph index for fast similarity search. - pgvector’s IVFFLAT index: an inverted file index for fast similarity search.\nTo learn more about Timescale Vector’s new DiskANN inspired index, see the Timescale Vector explainer blog for more details and performance benchmarks.\nImportant note: In PostgreSQL, each table can only have one index on a particular column. So if you’d like to test the performance of different index types, you can do so either by (1) creating multiple tables with different indexes, (2) creating multiple vector columns in the same table and creating different indexes on each column, or (3) by dropping and recreating the index on the same column and comparing results.\nLet’s look at how to create each type of index, starting with the StreamingDiskANN index.\n\n# Create a timescale vector (DiskANN) search index on the embedding column\nawait vec.create_embedding_index(client.DiskAnnIndex())\n\nTimescale Vector also supports HNSW and ivfflat indexes:\n\nawait vec.drop_embedding_index()\n\n# Create HNSW search index on the embedding column\nawait vec.create_embedding_index(client.HNSWIndex())\n\n\nawait vec.drop_embedding_index()\n\n# Create IVFFLAT search index on the embedding column\nawait vec.create_embedding_index(client.IvfflatIndex())\n\nIn general we recommend using the timescale vector or HNSW index for most use cases, as they are most useful for high dimension and large datasets.\n\nawait vec.drop_embedding_index()\nawait vec.create_embedding_index(client.DiskAnnIndex())",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#similarity-search-with-time-filtering",
    "href": "tsv_python_getting_started_tutorial.html#similarity-search-with-time-filtering",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "6. Similarity search with time filtering",
    "text": "6. Similarity search with time filtering\nWe’ll use Timescale Vector to find similar commits to a given query within a time range.\nA key use case for Timescale Vector is efficient time-based vector search. Timescale Vector enables this by automatically partitioning vectors (and associated metadata) by time. This allows you to efficiently query vectors by both similarity to a query vector and time.\nTime-based vector search functionality is helpful for applications like: - Storing and retrieving LLM response history (e.g. chatbots) - Finding the most recent embeddings that are similar to a query vector (e.g recent news). - Constraining similarity search to a relevant time range (e.g asking time-based questions about a knowledge base)\nLet’s look at how to run similarity searches with time range filters using the client.\n\nThe first step to using time filtering with Timescale Vector is to create a table with the time_partition_interval argument set to the desired time interval. This will automatically partition the table into time-based chunks to speed up queries. We completed this step in Part 1 above.\nNext, we ensure the id of our row is a uuid with a datetime portion that reflects the date and time we want to associated with the embedding. We completed this step in Part 2 above, where we used the uuid_from_time() method provided by the Timescale Vector library.\nFinally, we can run similarity searches with time range filters using the client. We’ll illustrate this below.\n\n\n# define search query\nquery_string = \"What's new with TimescaleDB functions?\"\nquery_embedding = get_embeddings(query_string)\n\n\n# Time filter variables for query\nstart_date = datetime(2023, 8, 1, 22, 10, 35) # Start date = 1 August 2023, 22:10:35\nend_date = datetime(2023, 8, 30, 22, 10, 35) # End date = 30 August 2023, 22:10:35\ntd = timedelta(days=7) # Time delta = 7 days\n\n\n# Method 1: Filter within a provided start date and end date.\nrecords_time_filtered = await vec.search(query_embedding,\n                        limit=3, \n                        uuid_time_filter=client.UUIDTimeRange(start_date, end_date))\n\n\nfor result in records_time_filtered:\n    print(\"-\" * 80)\n    print(result[client.SEARCH_RESULT_CONTENTS_IDX])\n\n--------------------------------------------------------------------------------\nSven Klemm&lt;sven@timescale.com&gt; Tue Aug 29 18:13:24 2023 +0200  e4facda540286b0affba47ccc63959fefe2a7b26 Add compatibility layer for _timescaledb_internal functions With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating. \n--------------------------------------------------------------------------------\nSven Klemm&lt;sven@timescale.com&gt; Tue Aug 29 18:13:24 2023 +0200  e4facda540286b0affba47ccc63959fefe2a7b26 Add compatibility layer for _timescaledb_internal functions With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating. \n--------------------------------------------------------------------------------\nDmitry Simonenko&lt;dmitry@timescale.com&gt; Thu Aug 3 14:30:23 2023 +0300  7aeed663b9c0f337b530fd6cad47704a51a9b2ec Feature flags for TimescaleDB features This PR adds several GUCs which allow to enable/disable major timescaledb features:  - enable_hypertable_create - enable_hypertable_compression - enable_cagg_create - enable_policy_create \n\n\nOnly vectors within the specified time range are returned. These queries are very efficient as they only need to search the relevant partitions.",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "tsv_python_getting_started_tutorial.html#resources-and-next-steps",
    "href": "tsv_python_getting_started_tutorial.html#resources-and-next-steps",
    "title": "Tutorial: Timescale Vector (PostgreSQL) Python Client Library",
    "section": "Resources and next steps",
    "text": "Resources and next steps\nTo continue your learning journey check out the following resources:\n\nTimescale Vector AI tutorials and guides\nTimescale Vector explainer blog and performance benchmarks for more details and performance benchmarks.\nUsing Timescale Vector with LangChain\nUsing Timescale Vector with LlamaIndex\n\nAnd finally, if you haven’t already, remember to claim your 90 days free of Timescale Vector by signing up here.",
    "crumbs": [
      "Tutorial: Timescale Vector (PostgreSQL) Python Client Library"
    ]
  },
  {
    "objectID": "pgvectorizer.html",
    "href": "pgvectorizer.html",
    "title": "PgVectorizer",
    "section": "",
    "text": "source\n\nVectorize\n\n Vectorize (service_url:str, table_name:str, schema_name:str='public',\n            id_column_name:str='id', work_queue_table_name:str=None,\n            trigger_name:str='track_changes_for_embedding',\n            trigger_name_fn:str=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n_ = load_dotenv(find_dotenv(), override=True)\nservice_url = os.environ['TIMESCALE_SERVICE_URL']\n\n\nwith psycopg2.connect(service_url) as conn:\n    with conn.cursor() as cursor:\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS blog (\n            id              SERIAL PRIMARY KEY NOT NULL,\n            title           TEXT NOT NULL,\n            author          TEXT NOT NULL,\n            contents        TEXT NOT NULL,\n            category        TEXT NOT NULL,\n            published_time  TIMESTAMPTZ NULL --NULL if not yet published\n        );\n        ''')\n        cursor.execute('''\n            insert into blog (title, author, contents, category, published_time) VALUES ('first', 'mat', 'first_post', 'personal', '2021-01-01');\n        ''')\n\n\nvectorizer = Vectorize(service_url, 'blog')\nvectorizer.register()\n# should be idempotent\nvectorizer.register()\n\n\nfrom langchain.docstore.document import Document\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom timescale_vector import client\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores.timescalevector import TimescaleVector\nfrom datetime import timedelta\n\n\ndef get_document(blog):\n    text_splitter = CharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n    )\n    docs = []\n    for chunk in text_splitter.split_text(blog['contents']):\n        content = f\"Author {blog['author']}, title: {blog['title']}, contents:{chunk}\"\n        metadata = {\n            \"id\": str(client.uuid_from_time(blog['published_time'])),\n            \"blog_id\": blog['id'], \n            \"author\": blog['author'], \n            \"category\": blog['category'],\n            \"published_time\": blog['published_time'].isoformat(),\n        }\n        docs.append(Document(page_content=content, metadata=metadata))\n    return docs\n\ndef embed_and_write(blog_instances, vectorizer):\n    TABLE_NAME = vectorizer.table_name_unquoted +\"_embedding\"\n    embedding = OpenAIEmbeddings()\n    vector_store = TimescaleVector(\n        collection_name=TABLE_NAME,\n        service_url=service_url,\n        embedding=embedding,\n        time_partition_interval=timedelta(days=30),\n    )\n\n    # delete old embeddings for all ids in the work queue\n    metadata_for_delete = [{\"blog_id\": blog['locked_id']} for blog in blog_instances]\n    vector_store.delete_by_metadata(metadata_for_delete)\n\n    documents = []\n    for blog in blog_instances:\n        # skip blogs that are not published yet, or are deleted (will be None because of left join)\n        if blog['published_time'] != None:\n            documents.extend(get_document(blog))\n\n    if len(documents) == 0:\n        return\n\n    texts = [d.page_content for d in documents]\n    metadatas = [d.metadata for d in documents]\n    ids = [d.metadata[\"id\"] for d in documents]\n    vector_store.add_texts(texts, metadatas, ids)\n\nvectorizer = Vectorize(service_url, 'blog')\nassert vectorizer.process(embed_and_write) == 1\nassert vectorizer.process(embed_and_write) == 0\n\nTABLE_NAME = \"blog_embedding\"\nembedding = OpenAIEmbeddings()\nvector_store = TimescaleVector(\n    collection_name=TABLE_NAME,\n    service_url=service_url,\n    embedding=embedding,\n    time_partition_interval=timedelta(days=30),\n)\n\nres = vector_store.similarity_search_with_score(\"first\", 10)\nassert len(res) == 1\n\n\nwith psycopg2.connect(service_url) as conn:\n    with conn.cursor() as cursor:\n        cursor.execute('''\n            insert into blog (title, author, contents, category, published_time) VALUES ('2', 'mat', 'second_post', 'personal', '2021-01-01');\n            insert into blog (title, author, contents, category, published_time) VALUES ('3', 'mat', 'third_post', 'personal', '2021-01-01');\n        ''')\nassert vectorizer.process(embed_and_write) == 2\nassert vectorizer.process(embed_and_write) == 0\n\nres = vector_store.similarity_search_with_score(\"first\", 10)\nassert len(res) == 3\n\nwith psycopg2.connect(service_url) as conn:\n    with conn.cursor() as cursor:\n        cursor.execute('''\n            DELETE FROM blog WHERE title = '3';\n        ''')\nassert vectorizer.process(embed_and_write) == 1\nassert vectorizer.process(embed_and_write) == 0\nres = vector_store.similarity_search_with_score(\"first\", 10)\nassert len(res) == 2\n\nres = vector_store.similarity_search_with_score(\"second\", 10)\nassert len(res) == 2\ncontent = res[0][0].page_content\nassert \"new version\" not in content\nwith psycopg2.connect(service_url) as conn:\n    with conn.cursor() as cursor:\n        cursor.execute('''\n            update blog set contents = 'second post new version' WHERE title = '2';\n        ''')\nassert vectorizer.process(embed_and_write) == 1\nassert vectorizer.process(embed_and_write) == 0\nres = vector_store.similarity_search_with_score(\"second\", 10)\nassert len(res) == 2\ncontent = res[0][0].page_content\nassert \"new version\" in content\n\n\nwith psycopg2.connect(service_url) as conn:\n    with conn.cursor() as cursor:\n        cursor.execute('''\n        CREATE TABLE IF NOT EXISTS test.blog_table_name_that_is_really_really_long_and_i_mean_long (\n            id              SERIAL PRIMARY KEY NOT NULL,\n            title           TEXT NOT NULL,\n            author          TEXT NOT NULL,\n            contents        TEXT NOT NULL,\n            category        TEXT NOT NULL,\n            published_time  TIMESTAMPTZ NULL --NULL if not yet published\n        );\n        ''')\n        cursor.execute('''\n            insert into test.blog_table_name_that_is_really_really_long_and_i_mean_long (title, author, contents, category, published_time) VALUES ('first', 'mat', 'first_post', 'personal', '2021-01-01');\n        ''')\n\nvectorizer = Vectorize(service_url, 'blog_table_name_that_is_really_really_long_and_i_mean_long', schema_name='test')\nassert vectorizer.process(embed_and_write) == 1\nassert vectorizer.process(embed_and_write) == 0",
    "crumbs": [
      "PgVectorizer"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Timescale Vector",
    "section": "",
    "text": "PostgreSQL++ for AI Applications.\nIf you prefer to use an LLM development or data framework, see Timescale Vector’s integrations with LangChain and LlamaIndex",
    "crumbs": [
      "Timescale Vector"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Timescale Vector",
    "section": "Install",
    "text": "Install\nTo install the main library use:\npip install timescale_vector\nWe also use dotenv in our examples for passing around secrets and keys. You can install that with:\npip install python-dotenv\nIf you run into installation errors related to the psycopg2 package, you will need to install some prerequisites. The timescale-vector package explicitly depends on psycopg2 (the non-binary version). This adheres to the advice provided by psycopg2. Building psycopg from source requires a few prerequisites to be installed. Make sure these are installed before trying to pip install timescale_vector.",
    "crumbs": [
      "Timescale Vector"
    ]
  },
  {
    "objectID": "index.html#basic-usage",
    "href": "index.html#basic-usage",
    "title": "Timescale Vector",
    "section": "Basic usage",
    "text": "Basic usage\nFirst, import all the necessary libraries:\n\nfrom dotenv import load_dotenv, find_dotenv\nimport os\nfrom timescale_vector import client\nimport uuid\nfrom datetime import datetime, timedelta\n\nLoad up your PostgreSQL credentials. Safest way is with a .env file:\n\n_ = load_dotenv(find_dotenv(), override=True) \nservice_url  = os.environ['TIMESCALE_SERVICE_URL']\n\nNext, create the client. In this tutorial, we will use the sync client. But we have an async client as well (with an identical interface that uses async functions).\nThe client constructor takes three required arguments:\n\n\n\n\n\n\n\nname\ndescription\n\n\n\n\nservice_url\nTimescale service URL / connection string\n\n\ntable_name\nName of the table to use for storing the embeddings. Think of this as the collection name\n\n\nnum_dimensions\nNumber of dimensions in the vector\n\n\n\nYou can also specify the schema name, distance type, primary key type, etc. as optional parameters. Please see the documentation for details.\n\nvec  = client.Sync(service_url, \"my_data\", 2)\n\nNext, create the tables for the collection:\n\nvec.create_tables()\n\nNext, insert some data. The data record contains:\n\nA UUID to uniquely identify the embedding\nA JSON blob of metadata about the embedding\nThe text the embedding represents\nThe embedding itself\n\nBecause this data includes UUIDs which become primary keys, we ingest with upserts.\n\nvec.upsert([\\\n    (uuid.uuid1(), {\"animal\": \"fox\"}, \"the brown fox\", [1.0,1.3]),\\\n    (uuid.uuid1(), {\"animal\": \"fox\", \"action\":\"jump\"}, \"jumped over the\", [1.0,10.8]),\\\n])\n\nYou can now create a vector index to speed up similarity search:\n\nvec.create_embedding_index(client.DiskAnnIndex())\n\nNow, you can query for similar items:\n\nvec.search([1.0, 9.0])\n\n[[UUID('4494c186-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('4494c12c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nThere are many search options which we will cover below in the Advanced search section.\nAs one example, we will return one item using a similarity search constrained by a metadata filter.\n\nvec.search([1.0, 9.0], limit=1, filter={\"action\": \"jump\"})\n\n[[UUID('4494c186-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\nThe returned records contain 5 fields:\n\n\n\nname\ndescription\n\n\n\n\nid\nThe UUID of the record\n\n\nmetadata\nThe JSON metadata associated with the record\n\n\ncontents\nthe text content that was embedded\n\n\nembedding\nThe vector embedding\n\n\ndistance\nThe distance between the query embedding and the vector\n\n\n\nYou can access the fields by simply using the record as a dictionary keyed on the field name:\n\nrecords = vec.search([1.0, 9.0], limit=1, filter={\"action\": \"jump\"})\n(records[0][\"id\"],records[0][\"metadata\"], records[0][\"contents\"], records[0][\"embedding\"], records[0][\"distance\"])\n\n(UUID('4494c186-4a0d-11ef-94a3-6ee10b77fd09'),\n {'action': 'jump', 'animal': 'fox'},\n 'jumped over the',\n array([ 1. , 10.8], dtype=float32),\n 0.00016793422934946456)\n\n\nYou can delete by ID:\n\nvec.delete_by_ids([records[0][\"id\"]])\n\nOr you can delete by metadata filters:\n\nvec.delete_by_metadata({\"action\": \"jump\"})\n\nTo delete all records use:\n\nvec.delete_all()",
    "crumbs": [
      "Timescale Vector"
    ]
  },
  {
    "objectID": "index.html#advanced-usage",
    "href": "index.html#advanced-usage",
    "title": "Timescale Vector",
    "section": "Advanced usage",
    "text": "Advanced usage\nIn this section, we will go into more detail about our feature. We will cover:\n\nSearch filter options - how to narrow your search by additional constraints\nIndexing - how to speed up your similarity queries\nTime-based partitioning - how to optimize similarity queries that filter on time\nSetting different distance types to use in distance calculations\n\n\nSearch options\nThe search function is very versatile and allows you to search for the right vector in a wide variety of ways. We’ll describe the search option in 3 parts:\n\nWe’ll cover basic similarity search.\nThen, we’ll describe how to filter your search based on the associated metadata.\nFinally, we’ll talk about filtering on time when time-partitioning is enabled.\n\nLet’s use the following data for our example:\n\nvec.upsert([\\\n    (uuid.uuid1(), {\"animal\":\"fox\", \"action\": \"sit\", \"times\":1}, \"the brown fox\", [1.0,1.3]),\\\n    (uuid.uuid1(),  {\"animal\":\"fox\", \"action\": \"jump\", \"times\":100}, \"jumped over the\", [1.0,10.8]),\\\n])\n\nThe basic query looks like:\n\nvec.search([1.0, 9.0])\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('456dbb6c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nYou could provide a limit for the number of items returned:\n\nvec.search([1.0, 9.0], limit=1)\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\n\nNarrowing your search by metadata\nWe have two main ways to filter results by metadata: - filters for equality matches on metadata. - predicates for complex conditions on metadata.\nFilters are more likely to be performant but are more limited in what they can express, so we suggest using those if your use case allows it.\n\nFilters\nYou could specify a match on the metadata as a dictionary where all keys have to match the provided values (keys not in the filter are unconstrained):\n\nvec.search([1.0, 9.0], limit=1, filter={\"action\": \"sit\"})\n\n[[UUID('456dbb6c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nYou can also specify a list of filter dictionaries, where an item is returned if it matches any dict:\n\nvec.search([1.0, 9.0], limit=2, filter=[{\"action\": \"jump\"}, {\"animal\": \"fox\"}])\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('456dbb6c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\n\n\nPredicates\nPredicates allow for more complex search conditions. For example, you could use greater than and less than conditions on numeric values.\n\nvec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"times\", \"&gt;\", 1))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\nPredicates objects are defined by the name of the metadata key, an operator, and a value.\nThe supported operators are: ==, !=, &lt;, &lt;=, &gt;, &gt;=\nThe type of the values determines the type of comparison to perform. For example, passing in \"Sam\" (a string) will do a string comparison while a 10 (an int) will perform an integer comparison while a 10.0 (float) will do a float comparison. It is important to note that using a value of \"10\" will do a string comparison as well so it’s important to use the right type. Supported Python types are: str, int, and float. One more example with a string comparison:\n\nvec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"action\", \"==\", \"jump\"))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\nThe real power of predicates is that they can also be combined using the & operator (for combining predicates with AND semantics) and |(for combining using OR semantic). So you can do:\n\nvec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"action\", \"==\", \"jump\") & client.Predicates(\"times\", \"&gt;\", 1))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\nJust for sanity, let’s show a case where no results are returned because or predicates:\n\nvec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"action\", \"==\", \"jump\") & client.Predicates(\"times\", \"==\", 1))\n\n[]\n\n\nAnd one more example where we define the predicates as a variable and use grouping with parenthesis:\n\nmy_predicates = client.Predicates(\"action\", \"==\", \"jump\") & (client.Predicates(\"times\", \"==\", 1) | client.Predicates(\"times\", \"&gt;\", 1))\nvec.search([1.0, 9.0], limit=2, predicates=my_predicates)\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\nWe also have some semantic sugar for combining many predicates with AND semantics. You can pass in multiple 3-tuples to Predicates:\n\nvec.search([1.0, 9.0], limit=2, predicates=client.Predicates((\"action\", \"==\", \"jump\"), (\"times\", \"&gt;\", 10)))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\n\n\n\nFilter your search by time\nWhen using time-partitioning(see below). You can very efficiently filter your search by time. Time-partitioning makes a timestamp embedded as part of the UUID-based ID associated with an embedding. Let us first create a collection with time partitioning and insert some data (one item from January 2018 and another in January 2019):\n\ntpvec = client.Sync(service_url, \"time_partitioned_table\", 2, time_partition_interval=timedelta(hours=6))\ntpvec.create_tables()\n\nspecific_datetime = datetime(2018, 1, 1, 12, 0, 0)\ntpvec.upsert([\\\n    (client.uuid_from_time(specific_datetime), {\"animal\":\"fox\", \"action\": \"sit\", \"times\":1}, \"the brown fox\", [1.0,1.3]),\\\n    (client.uuid_from_time(specific_datetime+timedelta(days=365)),  {\"animal\":\"fox\", \"action\": \"jump\", \"times\":100}, \"jumped over the\", [1.0,10.8]),\\\n])\n\nThen, you can filter using the timestamps by specifing a uuid_time_filter:\n\ntpvec.search([1.0, 9.0], limit=4, uuid_time_filter=client.UUIDTimeRange(specific_datetime, specific_datetime+timedelta(days=1)))\n\n[[UUID('33c52800-ef15-11e7-8a12-ea51d07b6447'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nA UUIDTimeRange can specify a start_date or end_date or both(as in the example above). Specifying only the start_date or end_date leaves the other end unconstrained.\n\ntpvec.search([1.0, 9.0], limit=4, uuid_time_filter=client.UUIDTimeRange(start_date=specific_datetime))\n\n[[UUID('ac8be800-0de6-11e9-a5fd-5a100e653c25'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('33c52800-ef15-11e7-8a12-ea51d07b6447'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nYou have the option to define the inclusivity of the start and end dates with the start_inclusive and end_inclusive parameters. Setting start_inclusive to true results in comparisons using the &gt;= operator, whereas setting it to false applies the &gt; operator. By default, the start date is inclusive, while the end date is exclusive. One example:\n\ntpvec.search([1.0, 9.0], limit=4, uuid_time_filter=client.UUIDTimeRange(start_date=specific_datetime, start_inclusive=False))\n\n[[UUID('ac8be800-0de6-11e9-a5fd-5a100e653c25'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456]]\n\n\nNotice how the results are different when we use the start_inclusive=False option because the first row has the exact timestamp specified by start_date.\nWe’ve also made it easy to integrate time filters using the filter and predicates parameters described above using special reserved key names to make it appear that the timestamps are part of your metadata. We found this useful when integrating with other systems that just want to specify a set of filters (often these are “auto retriever” type systems). The reserved key names are __start_date and __end_date for filters and __uuid_timestamp for predicates. Some examples below:\n\ntpvec.search([1.0, 9.0], limit=4, filter={ \"__start_date\": specific_datetime, \"__end_date\": specific_datetime+timedelta(days=1)})\n\n[[UUID('33c52800-ef15-11e7-8a12-ea51d07b6447'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\n\ntpvec.search([1.0, 9.0], limit=4, \n             predicates=client.Predicates(\"__uuid_timestamp\", \"&gt;=\", specific_datetime) & client.Predicates(\"__uuid_timestamp\", \"&lt;\", specific_datetime+timedelta(days=1)))\n\n[[UUID('33c52800-ef15-11e7-8a12-ea51d07b6447'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\n\n\n\nIndexing\nIndexing speeds up queries over your data. By default, we set up indexes to query your data by the UUID and the metadata.\nBut to speed up similarity search based on the embeddings, you have to create additional indexes.\nNote that if performing a query without an index, you will always get an exact result, but the query will be slow (it has to read all of the data you store for every query). With an index, your queries will be order-of-magnitude faster, but the results are approximate (because there are no known indexing techniques that are exact).\nNevertheless, there are excellent approximate algorithms. There are 3 different indexing algorithms available on the Timescale platform: Timescale Vector index, pgvector HNSW, and pgvector ivfflat. Below are the trade-offs between these algorithms:\n\n\n\n\n\n\n\n\n\nAlgorithm\nBuild speed\nQuery speed\nNeed to rebuild after updates\n\n\n\n\nStreamingDiskANN\nFast\nFastest\nNo\n\n\npgvector hnsw\nSlowest\nFaster\nNo\n\n\npgvector ivfflat\nFastest\nSlowest\nYes\n\n\n\nYou can see benchmarks on our blog.\nWe recommend using the Timescale Vector index for most use cases. This can be created with:\n\nvec.create_embedding_index(client.DiskAnnIndex())\n\nIndexes are created for a particular distance metric type. So it is important that the same distance metric is set on the client during index creation as it is during queries. See the distance type section below.\nEach of these indexes has a set of build-time options for controlling the speed/accuracy trade-off when creating the index and an additional query-time option for controlling accuracy during a particular query. We have smart defaults for all of these options but will also describe the details below so that you can adjust these options manually.\n\nStreamingDiskANN index\nThe StreamingDiskANN index from pgvectorscale is a graph-based algorithm that uses the DiskANN algorithm. You can read more about it on our blog announcing its release.\nTo create this index, run:\n\nvec.create_embedding_index(client.DiskAnnIndex())\n\nThe above command will create the index using smart defaults. There are a number of parameters you could tune to adjust the accuracy/speed trade-off.\nThe parameters you can set at index build time are:\n\n\n\nParameter name\nDescription\nDefault value\n\n\n\n\nstorage_layout\nmemory_optimized which uses SBQ to compress vector data or plain which stores data uncompressed\nmemory_optimized\n\n\nnum_neighbors\nSets the maximum number of neighbors per node. Higher values increase accuracy but make the graph traversal slower.\n50\n\n\nsearch_list_size\nThis is the S parameter used in the greedy search algorithm used during construction. Higher values improve graph quality at the cost of slower index builds.\n100\n\n\nmax_alpha\nIs the alpha parameter in the algorithm. Higher values improve graph quality at the cost of slower index builds.\n1.2\n\n\nnum_dimensions\nThe number of dimensions to index. By default, all dimensions are indexed. But you can also index less dimensions to make use of Matryoshka embeddings\n0 (all dimensions)\n\n\nnum_bits_per_dimension\nNumber of bits used to encode each dimension when using SBQ\n2 for less than 900 dimensions, 1 otherwise\n\n\n\nTo set these parameters, you could run:\n\nvec.create_embedding_index(client.DiskAnnIndex(num_neighbors=50, search_list_size=100, max_alpha=1.0, storage_layout=\"memory_optimized\", num_dimensions=0, num_bits_per_dimension=1))\n\nYou can also set a parameter to control the accuracy vs. query speed trade-off at query time. The parameter is set in the search() function using the query_params argment.\n\n\n\n\n\n\n\n\nParameter name\nDescription\nDefault value\n\n\n\n\nsearch_list_size\nThe number of additional candidates considered during the graph search.\n100\n\n\nrescore\nThe number of elements rescored (0 to disable rescoring)\n50\n\n\n\nWe suggest using the rescore parameter to fine-tune accuracy.\n\nvec.search([1.0, 9.0], limit=4, query_params=client.DiskAnnIndexParams(rescore=400, search_list_size=10))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('456dbb6c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nTo drop the index, run:\n\nvec.drop_embedding_index()\n\n\n\npgvector HNSW index\nPgvector provides a graph-based indexing algorithm based on the popular HNSW algorithm.\nTo create this index, run:\n\nvec.create_embedding_index(client.HNSWIndex())\n\nThe above command will create the index using smart defaults. There are a number of parameters you could tune to adjust the accuracy/speed trade-off.\nThe parameters you can set at index build time are:\n\n\n\n\n\n\n\n\nParameter name\nDescription\nDefault value\n\n\n\n\nm\nRepresents the maximum number of connections per layer. Think of these connections as edges created for each node during graph construction. Increasing m increases accuracy but also increases index build time and size.\n16\n\n\nef_construction\nRepresents the size of the dynamic candidate list for constructing the graph. It influences the trade-off between index quality and construction speed. Increasing ef_construction enables more accurate search results at the expense of lengthier index build times.\n64\n\n\n\nTo set these parameters, you could run:\n\nvec.create_embedding_index(client.HNSWIndex(m=16, ef_construction=64))\n\nYou can also set a parameter to control the accuracy vs. query speed trade-off at query time. The parameter is set in the search() function using the query_params argument. You can set the ef_search(default: 40). This parameter specifies the size of the dynamic candidate list used during search. Higher values improve query accuracy while making the query slower.\nYou can specify this value during search as follows:\n\nvec.search([1.0, 9.0], limit=4, query_params=client.HNSWIndexParams(ef_search=10))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('456dbb6c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nTo drop the index run:\n\nvec.drop_embedding_index()\n\n\n\npgvector ivfflat index\nPgvector provides a clustering-based indexing algorithm. Our blog post describes how it works in detail. It provides the fastest index-build speed but the slowest query speeds of any indexing algorithm.\nTo create this index, run:\n\nvec.create_embedding_index(client.IvfflatIndex())\n\nNote: ivfflat should never be created on empty tables because it needs to cluster data, and that only happens when an index is first created, not when new rows are inserted or modified. Also, if your table undergoes a lot of modifications, you will need to rebuild this index occasionally to maintain good accuracy. See our blog post for details.\nPgvector ivfflat has a lists index parameter that is automatically set with a smart default based on the number of rows in your table. If you know that you’ll have a different table size, you can specify the number of records to use for calculating the lists parameter as follows:\n\nvec.create_embedding_index(client.IvfflatIndex(num_records=1000000))\n\nYou can also set the lists parameter directly:\n\nvec.create_embedding_index(client.IvfflatIndex(num_lists=100))\n\nYou can also set a parameter to control the accuracy vs. query speed trade-off at query time. The parameter is set in the search() function using the query_params argument. You can set the probes. This parameter specifies the number of clusters searched during a query. It is recommended to set this parameter to sqrt(lists) where lists is the num_list parameter used above during index creation. Higher values improve query accuracy while making the query slower.\nYou can specify this value during search as follows:\n\nvec.search([1.0, 9.0], limit=4, query_params=client.IvfflatIndexParams(probes=10))\n\n[[UUID('456dbbbc-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n  'jumped over the',\n  array([ 1. , 10.8], dtype=float32),\n  0.00016793422934946456],\n [UUID('456dbb6c-4a0d-11ef-94a3-6ee10b77fd09'),\n  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n  'the brown fox',\n  array([1. , 1.3], dtype=float32),\n  0.14489260377438218]]\n\n\nTo drop the index, run:\n\nvec.drop_embedding_index()\n\n\n\n\nTime partitioning\nIn many use cases where you have many embeddings, time is an important component associated with the embeddings. For example, when embedding news stories, you often search by time as well as similarity (e.g., stories related to Bitcoin in the past week or stories about Clinton in November 2016).\nYet, traditionally, searching by two components “similarity” and “time” is challenging for Approximate Nearest Neighbor (ANN) indexes and makes the similarity-search index less effective.\nOne approach to solving this is partitioning the data by time and creating ANN indexes on each partition individually. Then, during search, you can:\n\nStep 1: filter our partitions that don’t match the time predicate.\nStep 2: perform the similarity search on all matching partitions.\nStep 3: combine all the results from each partition in step 2, rerank, and filter out results by time.\n\nStep 1 makes the search a lot more efficient by filtering out whole swaths of data in one go.\nTimescale-vector supports time partitioning using TimescaleDB’s hypertables. To use this feature, simply indicate the length of time for each partition when creating the client:\n\nfrom datetime import timedelta\nfrom datetime import datetime\n\n\nvec = client.Async(service_url, \"my_data_with_time_partition\", 2, time_partition_interval=timedelta(hours=6))\nawait vec.create_tables()\n\nThen, insert data where the IDs use UUIDs v1 and the time component of the UUID specifies the time of the embedding. For example, to create an embedding for the current time, simply do:\n\nid = uuid.uuid1()\nawait vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n\nTo insert data for a specific time in the past, create the UUID using our uuid_from_time function\n\nspecific_datetime = datetime(2018, 8, 10, 15, 30, 0)\nawait vec.upsert([(client.uuid_from_time(specific_datetime), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n\nYou can then query the data by specifying a uuid_time_filter in the search call:\n\nrec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=client.UUIDTimeRange(specific_datetime-timedelta(days=7), specific_datetime+timedelta(days=7)))\n\n\n\nDistance metrics\nBy default, we use cosine distance to measure how similarly an embedding is to a given query. In addition to cosine distance, we also support Euclidean/L2 distance. The distance type is set when creating the client using the distance_type parameter. For example, to use the Euclidean distance metric, you can create the client with:\n\nvec  = client.Sync(service_url, \"my_data\", 2, distance_type=\"euclidean\")\n\nValid values for distance_type are cosine and euclidean.\nIt is important to note that you should use consistent distance types on clients that create indexes and perform queries. That is because an index is only valid for one particular type of distance measure.\nPlease note the Timescale Vector index only supports cosine distance at this time.",
    "crumbs": [
      "Timescale Vector"
    ]
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "Timescale Vector",
    "section": "Development",
    "text": "Development\nThis project is developed with nbdev. Please see that website for the development process.",
    "crumbs": [
      "Timescale Vector"
    ]
  }
]