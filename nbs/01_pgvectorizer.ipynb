{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PgVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pgvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import psycopg2.pool\n",
    "from contextlib import contextmanager\n",
    "import psycopg2.extras\n",
    "import pgvector.psycopg2\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from timescale_vector import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _create_ident(base: str, suffix: str):\n",
    "     if len(base) + len(suffix) > 62:\n",
    "            base = base[:62 - len(suffix)]\n",
    "     return re.sub(r'[^a-zA-Z0-9_]', '_', f\"{base}_{suffix}\")\n",
    "\n",
    "class Vectorize:\n",
    "    def __init__(self,\n",
    "                 service_url: str, \n",
    "                 table_name: str,\n",
    "                 schema_name: str='public',\n",
    "                 id_column_name: str='id', \n",
    "                 work_queue_table_name: str=None, \n",
    "                 trigger_name: str='track_changes_for_embedding', \n",
    "                 trigger_name_fn: str=None) -> None:\n",
    "        self.service_url = service_url\n",
    "        self.table_name_unquoted = table_name\n",
    "        self.schema_name_unquoted = schema_name\n",
    "        self.table_name = client.QueryBuilder._quote_ident(table_name)\n",
    "        self.schema_name = client.QueryBuilder._quote_ident(schema_name)\n",
    "        self.id_column_name = client.QueryBuilder._quote_ident(id_column_name)\n",
    "        if work_queue_table_name is None:\n",
    "            work_queue_table_name = _create_ident(table_name, 'embedding_work_queue')\n",
    "        self.work_queue_table_name = client.QueryBuilder._quote_ident(work_queue_table_name)\n",
    "        \n",
    "        self.trigger_name = client.QueryBuilder._quote_ident(trigger_name)\n",
    "\n",
    "        if trigger_name_fn is None:\n",
    "            trigger_name_fn = _create_ident(table_name, 'wq_for_embedding')\n",
    "        self.trigger_name_fn = client.QueryBuilder._quote_ident(trigger_name_fn) \n",
    "\n",
    "\n",
    "    def register(self):        \n",
    "        with psycopg2.connect(self.service_url) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT to_regclass('{self.schema_name}.{self.work_queue_table_name}') is not null; \n",
    "                \"\"\")\n",
    "                table_exists = cursor.fetchone()[0]\n",
    "                if table_exists:\n",
    "                    return\n",
    "                \n",
    "                cursor.execute(f\"\"\"\n",
    "                    CREATE TABLE {self.schema_name}.{self.work_queue_table_name} (\n",
    "                        id int\n",
    "                    );\n",
    "\n",
    "                    CREATE INDEX ON {self.schema_name}.{self.work_queue_table_name}(id);\n",
    "\n",
    "                    CREATE OR REPLACE FUNCTION {self.schema_name}.{self.trigger_name_fn}() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ \n",
    "                    BEGIN \n",
    "                        IF (TG_OP = 'DELETE') THEN\n",
    "                            INSERT INTO {self.work_queue_table_name} \n",
    "                            VALUES (OLD.{self.id_column_name});\n",
    "                        ELSE\n",
    "                            INSERT INTO {self.work_queue_table_name} \n",
    "                            VALUES (NEW.{self.id_column_name});\n",
    "                        END IF;\n",
    "                        RETURN NULL;\n",
    "                    END; \n",
    "                    $$;\n",
    "\n",
    "                    CREATE TRIGGER {self.trigger_name} \n",
    "                    AFTER INSERT OR UPDATE OR DELETE\n",
    "                    ON {self.schema_name}.{self.table_name} \n",
    "                    FOR EACH ROW EXECUTE PROCEDURE {self.schema_name}.{self.trigger_name_fn}();\n",
    "\n",
    "                    INSERT INTO {self.schema_name}.{self.work_queue_table_name} SELECT {self.id_column_name} FROM {self.schema_name}.{self.table_name};\n",
    "                \"\"\")\n",
    "\n",
    "    def process(self, embed_and_write_cb, batch_size:int=10, autoregister=True):\n",
    "        if autoregister:\n",
    "            self.register()\n",
    "            \n",
    "        with psycopg2.connect(self.service_url) as conn:\n",
    "            with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cursor:\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT to_regclass('{self.schema_name}.{self.work_queue_table_name}')::oid; \n",
    "                \"\"\")\n",
    "                table_oid = cursor.fetchone()[0]\n",
    "            \n",
    "                cursor.execute(f\"\"\"\n",
    "                    WITH selected_rows AS (\n",
    "                        SELECT id\n",
    "                        FROM {self.schema_name}.{self.work_queue_table_name}\n",
    "                        LIMIT {int(batch_size)}\n",
    "                        FOR UPDATE SKIP LOCKED\n",
    "                    ), \n",
    "                    locked_items AS (\n",
    "                        SELECT id, pg_try_advisory_xact_lock({int(table_oid)}, id) AS locked\n",
    "                        FROM (SELECT DISTINCT id FROM selected_rows ORDER BY id) as ids\n",
    "                    ),\n",
    "                    deleted_rows AS (\n",
    "                        DELETE FROM {self.schema_name}.{self.work_queue_table_name}\n",
    "                        WHERE id IN (SELECT id FROM locked_items WHERE locked = true ORDER BY id)\n",
    "                    )\n",
    "                    SELECT locked_items.id as locked_id, {self.table_name}.*\n",
    "                    FROM locked_items\n",
    "                    LEFT JOIN {self.schema_name}.{self.table_name} ON {self.table_name}.{self.id_column_name} = locked_items.id\n",
    "                    WHERE locked = true\n",
    "                    ORDER BY locked_items.id\n",
    "                \"\"\")\n",
    "                res = cursor.fetchall()\n",
    "                if len(res) > 0:\n",
    "                    embed_and_write_cb(res, self)\n",
    "                return len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv(), override=True)\n",
    "service_url = os.environ['TIMESCALE_SERVICE_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| hide\n",
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for item in ['blog', 'blog_embedding_work_queue', 'blog_embedding']:\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {item};\")\n",
    "        \n",
    "        for item in ['public','test']:\n",
    "            cursor.execute(f\"DROP SCHEMA IF EXISTS {item} CASCADE;\")\n",
    "            cursor.execute(f\"CREATE SCHEMA {item};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS blog (\n",
    "            id              SERIAL PRIMARY KEY NOT NULL,\n",
    "            title           TEXT NOT NULL,\n",
    "            author          TEXT NOT NULL,\n",
    "            contents        TEXT NOT NULL,\n",
    "            category        TEXT NOT NULL,\n",
    "            published_time  TIMESTAMPTZ NULL --NULL if not yet published\n",
    "        );\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            insert into blog (title, author, contents, category, published_time) VALUES ('first', 'mat', 'first_post', 'personal', '2021-01-01');\n",
    "        ''')\n",
    "\n",
    "\n",
    "vectorizer = Vectorize(service_url, 'blog')\n",
    "vectorizer.register()\n",
    "# should be idempotent\n",
    "vectorizer.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from timescale_vector import client\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.timescalevector import TimescaleVector\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(blog):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    docs = []\n",
    "    for chunk in text_splitter.split_text(blog['contents']):\n",
    "        content = f\"Author {blog['author']}, title: {blog['title']}, contents:{chunk}\"\n",
    "        metadata = {\n",
    "            \"id\": str(client.uuid_from_time(blog['published_time'])),\n",
    "            \"blog_id\": blog['id'], \n",
    "            \"author\": blog['author'], \n",
    "            \"category\": blog['category'],\n",
    "            \"published_time\": blog['published_time'].isoformat(),\n",
    "        }\n",
    "        docs.append(Document(page_content=content, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def embed_and_write(blog_instances, vectorizer):\n",
    "    TABLE_NAME = vectorizer.table_name_unquoted +\"_embedding\"\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vector_store = TimescaleVector(\n",
    "        collection_name=TABLE_NAME,\n",
    "        service_url=service_url,\n",
    "        embedding=embedding,\n",
    "        time_partition_interval=timedelta(days=30),\n",
    "    )\n",
    "\n",
    "    # delete old embeddings for all ids in the work queue\n",
    "    metadata_for_delete = [{\"blog_id\": blog['locked_id']} for blog in blog_instances]\n",
    "    vector_store.delete_by_metadata(metadata_for_delete)\n",
    "\n",
    "    documents = []\n",
    "    for blog in blog_instances:\n",
    "        # skip blogs that are not published yet, or are deleted (will be None because of left join)\n",
    "        if blog['published_time'] != None:\n",
    "            documents.extend(get_document(blog))\n",
    "\n",
    "    if len(documents) == 0:\n",
    "        return\n",
    "\n",
    "    texts = [d.page_content for d in documents]\n",
    "    metadatas = [d.metadata for d in documents]\n",
    "    ids = [d.metadata[\"id\"] for d in documents]\n",
    "    vector_store.add_texts(texts, metadatas, ids)\n",
    "\n",
    "vectorizer = Vectorize(service_url, 'blog')\n",
    "assert vectorizer.process(embed_and_write) == 1\n",
    "assert vectorizer.process(embed_and_write) == 0\n",
    "\n",
    "TABLE_NAME = \"blog_embedding\"\n",
    "embedding = OpenAIEmbeddings()\n",
    "vector_store = TimescaleVector(\n",
    "    collection_name=TABLE_NAME,\n",
    "    service_url=service_url,\n",
    "    embedding=embedding,\n",
    "    time_partition_interval=timedelta(days=30),\n",
    ")\n",
    "\n",
    "res = vector_store.similarity_search_with_score(\"first\", 10)\n",
    "assert len(res) == 1\n",
    "\n",
    "\n",
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            insert into blog (title, author, contents, category, published_time) VALUES ('2', 'mat', 'second_post', 'personal', '2021-01-01');\n",
    "            insert into blog (title, author, contents, category, published_time) VALUES ('3', 'mat', 'third_post', 'personal', '2021-01-01');\n",
    "        ''')\n",
    "assert vectorizer.process(embed_and_write) == 2\n",
    "assert vectorizer.process(embed_and_write) == 0\n",
    "\n",
    "res = vector_store.similarity_search_with_score(\"first\", 10)\n",
    "assert len(res) == 3\n",
    "\n",
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            DELETE FROM blog WHERE title = '3';\n",
    "        ''')\n",
    "assert vectorizer.process(embed_and_write) == 1\n",
    "assert vectorizer.process(embed_and_write) == 0\n",
    "res = vector_store.similarity_search_with_score(\"first\", 10)\n",
    "assert len(res) == 2\n",
    "\n",
    "res = vector_store.similarity_search_with_score(\"second\", 10)\n",
    "assert len(res) == 2\n",
    "content = res[0][0].page_content\n",
    "assert \"new version\" not in content\n",
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            update blog set contents = 'second post new version' WHERE title = '2';\n",
    "        ''')\n",
    "assert vectorizer.process(embed_and_write) == 1\n",
    "assert vectorizer.process(embed_and_write) == 0\n",
    "res = vector_store.similarity_search_with_score(\"second\", 10)\n",
    "assert len(res) == 2\n",
    "content = res[0][0].page_content\n",
    "assert \"new version\" in content\n",
    "\n",
    "\n",
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS test.blog_table_name_that_is_really_really_long_and_i_mean_long (\n",
    "            id              SERIAL PRIMARY KEY NOT NULL,\n",
    "            title           TEXT NOT NULL,\n",
    "            author          TEXT NOT NULL,\n",
    "            contents        TEXT NOT NULL,\n",
    "            category        TEXT NOT NULL,\n",
    "            published_time  TIMESTAMPTZ NULL --NULL if not yet published\n",
    "        );\n",
    "        ''')\n",
    "        cursor.execute('''\n",
    "            insert into test.blog_table_name_that_is_really_really_long_and_i_mean_long (title, author, contents, category, published_time) VALUES ('first', 'mat', 'first_post', 'personal', '2021-01-01');\n",
    "        ''')\n",
    "\n",
    "vectorizer = Vectorize(service_url, 'blog_table_name_that_is_really_really_long_and_i_mean_long', schema_name='test')\n",
    "assert vectorizer.process(embed_and_write) == 1\n",
    "assert vectorizer.process(embed_and_write) == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
