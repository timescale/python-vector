{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timescale Vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostgreSQL++ for AI Applications.\n",
    "\n",
    "- [Signup for Timescale Vector](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=github&utm_medium=direct): Get 90 days free to try Timescale Vector on the Timescale cloud data platform. There is no self-managed version at this time.\n",
    "- [Documentation](https://timescale.github.io/python-vector/): Learn the key features of Timescale Vector and how to use them.\n",
    "- [Getting Started Tutorial](https://timescale.github.io/python-vector/tsv_python_getting_started_tutorial.html): Learn how to use Timescale Vector for semantic search on a real-world dataset.\n",
    "- [Learn more](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/?utm_campaign=vectorlaunch&utm_source=github&utm_medium=direct): Learn more about Timescale Vector, how it works and why we built it.\n",
    "\n",
    "\n",
    "If you prefer to use an LLM development or data framework, see Timescale Vector's integrations with [LangChain](https://python.langchain.com/docs/integrations/vectorstores/timescalevector) and [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/examples/vector_stores/Timescalevector.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the main library use:\n",
    "\n",
    "```sh\n",
    "pip install timescale_vector\n",
    "```\n",
    "\n",
    "We also use `dotenv` in our examples for passing around secrets and keys. You can install that with:\n",
    "\n",
    "```sh\n",
    "pip install python-dotenv\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "First, import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from timescale_vector import client\n",
    "import uuid\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up your PostgreSQL credentials. Safest way is with a .env file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv(), override=True) \n",
    "service_url  = os.environ['TIMESCALE_SERVICE_URL'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the client. In this tutorial, we will use the sync client. But we have an async client as well (with an identical interface that uses async functions).\n",
    "\n",
    "The client constructor takes three required arguments: \n",
    "\n",
    "\n",
    "| name        | description                               |\n",
    "|-------------|-------------------------------------------|\n",
    "| service_url | Timescale service URL / connection string |\n",
    "| table_name  | Name of the table to use for storing the embeddings. Think of this as the collection name |\n",
    "| num_dimensions | Number of dimensions in the vector | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import asyncpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "con = await asyncpg.connect(service_url)\n",
    "await con.execute(\"DROP TABLE IF EXISTS my_data;\")\n",
    "await con.execute(\"DROP TABLE IF EXISTS my_data_with_time_partition;\")\n",
    "await con.execute(\"DROP TABLE IF EXISTS time_partitioned_table;\")\n",
    "await con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec  = client.Sync(service_url, \"my_data\", 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the tables for the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_tables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, insert some data. The data record contains:\n",
    "\n",
    "* A UUID to uniquely identify the embedding\n",
    "* A JSON blob of metadata about the embedding\n",
    "* The text the embedding represents\n",
    "* The embedding itself\n",
    "\n",
    "Because this data includes UUIDs which become primary keys, we ingest with upserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.upsert([\\\n",
    "    (uuid.uuid1(), {\"animal\": \"fox\"}, \"the brown fox\", [1.0,1.3]),\\\n",
    "    (uuid.uuid1(), {\"animal\": \"fox\", \"action\":\"jump\"}, \"jumped over the\", [1.0,10.8]),\\\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create a vector index to speed up similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.TimescaleVectorIndex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can query for similar items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('73d05df0-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456],\n",
       " [UUID('73d05d6e-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many search options which we will cover below in the `Advanced search` section.\n",
    "\n",
    "As one example, we will return one item using a similarity search constrained by a metadata filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('73d05df0-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=1, filter={\"action\": \"jump\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned records contain 5 fields:\n",
    "\n",
    "| name | description |\n",
    "|------|-------------|\n",
    "| id | The UUID of the record|\n",
    "| metadata | The JSON metadata associated with the record|\n",
    "| contents  | the text content that was embedded |\n",
    "| embedding | The vector embedding |\n",
    "| distance | The distance between the query embedding and the vector | \n",
    "\n",
    "You can access the fields by simply using the record as a dictionary keyed on the field name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(UUID('73d05df0-84c1-11ee-98da-6ee10b77fd08'),\n",
       " {'action': 'jump', 'animal': 'fox'},\n",
       " 'jumped over the',\n",
       " array([ 1. , 10.8], dtype=float32),\n",
       " 0.00016793422934946456)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = vec.search([1.0, 9.0], limit=1, filter={\"action\": \"jump\"})\n",
    "(records[0][\"id\"],records[0][\"metadata\"], records[0][\"contents\"], records[0][\"embedding\"], records[0][\"distance\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can delete by ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.delete_by_ids([records[0][\"id\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can delete by metadata filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.delete_by_metadata({\"action\": \"jump\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete all records use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.delete_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced usage\n",
    "\n",
    "In this section, we will go into more detail about our feature. We will cover:\n",
    "\n",
    "1. Search filter options - how to narrow your search by additional constraints\n",
    "1. Indexing - how to speed up your similarity queries\n",
    "1. Time-based partitioning - how to optimize similarity queries that filter on time\n",
    "1. Setting different distance types to use in distance calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search options\n",
    "\n",
    "The `search` function is very versatile and allows you to search for the right vector in a wide variety of ways. We'll describe the search option in 3 parts:\n",
    "\n",
    "1. We'll cover basic similarity search.\n",
    "1. Then, we'll describe how to filter your search based on the associated metadata.\n",
    "1. Finally, we'll talk about filtering on time when time-partitioning is enabled.\n",
    "\n",
    "\n",
    "\n",
    "Let's use the following data for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.upsert([\\\n",
    "    (uuid.uuid1(), {\"animal\":\"fox\", \"action\": \"sit\", \"times\":1}, \"the brown fox\", [1.0,1.3]),\\\n",
    "    (uuid.uuid1(),  {\"animal\":\"fox\", \"action\": \"jump\", \"times\":100}, \"jumped over the\", [1.0,10.8]),\\\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic query looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456],\n",
       " [UUID('7487af14-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could provide a limit for the number of items returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrowing your search by metadata\n",
    "\n",
    "We have two main ways to filter results by metadata:\n",
    "- `filters` for equality matches on metadata.\n",
    "- `predicates` for complex conditions on metadata.\n",
    "\n",
    "Filters are more likely to be performant but are more limited in what they can express, so we suggest using those if your use case allows it.\n",
    "\n",
    "##### Filters\n",
    "\n",
    "You could specify a match on the metadata as a dictionary where all keys have to match the provided values (keys not in the filter are unconstrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af14-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=1, filter={\"action\": \"sit\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a list of filter dictionaries, where an item is returned if it matches any dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456],\n",
       " [UUID('7487af14-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=2, filter=[{\"action\": \"jump\"}, {\"animal\": \"fox\"}])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicates\n",
    "\n",
    "Predicates allow for more complex search conditions. For example, you could use greater than and less than conditions on numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"times\", \">\", 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Predicates` objects are defined by the name of the metadata key, an operator, and a value.\n",
    "\n",
    "The supported operators are: `==`, `!=`, `<`, `<=`, `>`, `>=`\n",
    "\n",
    "The type of the values determines the type of comparison to perform. For example, passing in `\"Sam\"` (a string) will do a string comparison while a `10` (an int) will perform an integer comparison while a `10.0` (float) will do a float comparison. It is important to note that using a value of `\"10\"` will do a string comparison as well so it's important to use the right type. Supported Python types are: `str`, `int`, and `float`. One more example with a string comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"action\", \"==\", \"jump\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real power of predicates is that they can also be combined using the `&` operator (for combining predicates with AND semantics) and `|`(for combining using OR semantic). So you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"action\", \"==\", \"jump\") & client.Predicates(\"times\", \">\", 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for sanity, let's show a case where no results are returned because or predicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=2, predicates=client.Predicates(\"action\", \"==\", \"jump\") & client.Predicates(\"times\", \"==\", 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one more example where we define the predicates as a variable and use grouping with parenthesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_predicates = client.Predicates(\"action\", \"==\", \"jump\") & (client.Predicates(\"times\", \"==\", 1) | client.Predicates(\"times\", \">\", 1))\n",
    "vec.search([1.0, 9.0], limit=2, predicates=my_predicates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have some semantic sugar for combining many predicates with AND semantics. You can pass in multiple 3-tuples to `Predicates`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('7487af96-84c1-11ee-98da-6ee10b77fd08'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.search([1.0, 9.0], limit=2, predicates=client.Predicates((\"action\", \"==\", \"jump\"), (\"times\", \">\", 10)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter your search by time\n",
    "\n",
    "When using `time-partitioning`(see below). You can very efficiently filter your search by time. Time-partitioning makes a timestamp embedded as part of the UUID-based ID associated with an embedding. Let us first create a collection with time partitioning and insert some data (one item from January 2018 and another in January 2019):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpvec = client.Sync(service_url, \"time_partitioned_table\", 2, time_partition_interval=timedelta(hours=6))\n",
    "tpvec.create_tables()\n",
    "\n",
    "specific_datetime = datetime(2018, 1, 1, 12, 0, 0)\n",
    "tpvec.upsert([\\\n",
    "    (client.uuid_from_time(specific_datetime), {\"animal\":\"fox\", \"action\": \"sit\", \"times\":1}, \"the brown fox\", [1.0,1.3]),\\\n",
    "    (client.uuid_from_time(specific_datetime+timedelta(days=365)),  {\"animal\":\"fox\", \"action\": \"jump\", \"times\":100}, \"jumped over the\", [1.0,10.8]),\\\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can filter using the timestamps by specifing a `uuid_time_filter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('33c52800-ef15-11e7-be03-4f1f9a1bde5a'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpvec.search([1.0, 9.0], limit=4, uuid_time_filter=client.UUIDTimeRange(specific_datetime, specific_datetime+timedelta(days=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `UUIDTimeRange` can specify a start_date or end_date or both(as in the example above). Specifying only the start_date or end_date leaves the other end unconstrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('ac8be800-0de6-11e9-889a-5eec84ba8a7b'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456],\n",
       " [UUID('33c52800-ef15-11e7-be03-4f1f9a1bde5a'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpvec.search([1.0, 9.0], limit=4, uuid_time_filter=client.UUIDTimeRange(start_date=specific_datetime))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have the option to define the inclusivity of the start and end dates with the `start_inclusive` and `end_inclusive` parameters. Setting `start_inclusive` to true results in comparisons using the `>=` operator, whereas setting it to false applies the `>` operator. By default, the start date is inclusive, while the end date is exclusive. One example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('ac8be800-0de6-11e9-889a-5eec84ba8a7b'),\n",
       "  {'times': 100, 'action': 'jump', 'animal': 'fox'},\n",
       "  'jumped over the',\n",
       "  array([ 1. , 10.8], dtype=float32),\n",
       "  0.00016793422934946456]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpvec.search([1.0, 9.0], limit=4, uuid_time_filter=client.UUIDTimeRange(start_date=specific_datetime, start_inclusive=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the results are different when we use the `start_inclusive=False` option because the first row has the exact timestamp specified by `start_date`.\n",
    "\n",
    "We've also made it easy to integrate time filters using the `filter` and `predicates` parameters described above using special reserved key names to make it appear that the timestamps are part of your metadata. We found this useful when integrating with other systems that just want to specify a set of filters (often these are \"auto retriever\" type systems). The reserved key names are `__start_date` and `__end_date` for filters and `__uuid_timestamp` for predicates. Some examples below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('33c52800-ef15-11e7-be03-4f1f9a1bde5a'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpvec.search([1.0, 9.0], limit=4, filter={ \"__start_date\": specific_datetime, \"__end_date\": specific_datetime+timedelta(days=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[UUID('33c52800-ef15-11e7-be03-4f1f9a1bde5a'),\n",
       "  {'times': 1, 'action': 'sit', 'animal': 'fox'},\n",
       "  'the brown fox',\n",
       "  array([1. , 1.3], dtype=float32),\n",
       "  0.14489260377438218]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpvec.search([1.0, 9.0], limit=4, \n",
    "             predicates=client.Predicates(\"__uuid_timestamp\", \">\", specific_datetime) & client.Predicates(\"__uuid_timestamp\", \"<\", specific_datetime+timedelta(days=1)))\n",
    "             "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Indexing speeds up queries over your data. By default, we set up indexes to query your data by the UUID and the metadata.\n",
    "\n",
    "But to speed up similarity search based on the embeddings, you have to create additional indexes.\n",
    "\n",
    "Note that if performing a query without an index, you will always get an exact result, but the query will be slow (it has to read all of the data you store for every query). With an index, your queries will be order-of-magnitude faster, but the results are approximate (because there are no known indexing techniques that are exact).\n",
    "\n",
    "Nevertheless, there are excellent approximate algorithms. There are 3 different indexing algorithms available on the Timescale platform: Timescale Vector index, pgvector HNSW, and pgvector ivfflat. Below are the trade-offs between these algorithms:\n",
    "\n",
    "| Algorithm       | Build speed | Query speed | Need to rebuild after updates |\n",
    "|------------------|-------------|-------------|-------------------------------|\n",
    "| timescale vector | Slow        | Fastest     | No                            |\n",
    "| pgvector hnsw    | Slowest     | Faster      | No                            |\n",
    "| pgvector ivfflat | Fastest     | Slowest     | Yes                           |\n",
    "\n",
    "\n",
    "You can see [benchmarks](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/) on our blog.\n",
    "\n",
    "We recommend using the Timescale Vector index for most use cases. This can be created with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.TimescaleVectorIndex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vec.drop_embedding_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes are created for a particular distance metric type. So it is important that the same distance metric is set on the client during index creation as it is during queries. See the `distance type` section below.\n",
    "\n",
    "Each of these indexes has a set of build-time options for controlling the speed/accuracy trade-off when creating the index and an additional query-time option for controlling accuracy during a particular query. We have smart defaults for all of these options but will also describe the details below so that you can adjust these options manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timescale Vector index\n",
    "\n",
    "The Timescale Vector index is a graph-based algorithm that uses the [DiskANN](https://github.com/microsoft/DiskANN) algorithm. You can read more about it on our [blog](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/) announcing its release.\n",
    "\n",
    "\n",
    "To create this index, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.TimescaleVectorIndex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command will create the index using smart defaults.  There are a number of parameters you could tune to adjust the accuracy/speed trade-off.\n",
    "\n",
    "The parameters you can set at index build time are:\n",
    "\n",
    "| Parameter name   | Description                                                                                                                                                    | Default value |\n",
    "|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| num_neighbors    | Sets the maximum number of neighbors per node.  Higher values increase accuracy but make the graph traversal slower.                                           | 50            |\n",
    "| search_list_size | This is the S parameter used in the greedy search algorithm used during construction.  Higher values improve graph quality at the cost of slower index builds. | 100           |\n",
    "| max_alpha        | Is the alpha parameter in the algorithm.  Higher values improve graph quality at the cost of slower index builds.                                              | 1.0           |\n",
    "\n",
    "To set these parameters, you could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vec.drop_embedding_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.TimescaleVectorIndex(num_neighbors=50, search_list_size=100, max_alpha=1.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set a parameter to control the accuracy vs. query speed trade-off at query time. The parameter is set in the `search()` function using the `query_params` argment. You can set the `search_list_size`(default: 100). This is the number of additional candidates considered during the graph search at query time. Higher values improve query accuracy while making the query slower.\n",
    "\n",
    "You can specify this value during search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec.search([1.0, 9.0], limit=4, query_params=TimescaleVectorIndexParams(search_list_size=10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop the index, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.drop_embedding_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pgvector HNSW index\n",
    "\n",
    "Pgvector provides a graph-based indexing algorithm based on the popular [HNSW algorithm](https://arxiv.org/abs/1603.09320).  \n",
    "\n",
    "To create this index, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.HNSWIndex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command will create the index using smart defaults.  There are a number of parameters you could tune to adjust the accuracy/speed trade-off.\n",
    "\n",
    "The parameters you can set at index build time are:\n",
    "\n",
    "| Parameter name   | Description                                                                                                                                                    | Default value |\n",
    "|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| m    | Represents the maximum number of connections per layer. Think of these connections as edges created for each node during graph construction. Increasing m increases accuracy but also increases index build time and size.                                       | 16            |\n",
    "| ef_construction | Represents the size of the dynamic candidate list for constructing the graph. It influences the trade-off between index quality and construction speed. Increasing ef_construction enables more accurate search results at the expense of lengthier index build times. | 64           |\n",
    "\n",
    "To set these parameters, you could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vec.drop_embedding_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.HNSWIndex(m=16, ef_construction=64))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set a parameter to control the accuracy vs. query speed trade-off at query time. The parameter is set in the `search()` function using the `query_params` argument. You can set the `ef_search`(default: 40). This parameter specifies the size of the dynamic candidate list used during search. Higher values improve query accuracy while making the query slower.\n",
    "\n",
    "You can specify this value during search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec.search([1.0, 9.0], limit=4, query_params=HNSWIndexParams(ef_search=10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop the index run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.drop_embedding_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pgvector ivfflat index\n",
    "\n",
    "Pgvector provides a clustering-based indexing algorithm. Our [blog post](https://www.timescale.com/blog/nearest-neighbor-indexes-what-are-ivfflat-indexes-in-pgvector-and-how-do-they-work/) describes how it works in detail. It provides the fastest index-build speed but the slowest query speeds of any indexing algorithm.\n",
    "\n",
    "To create this index, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.IvfflatIndex())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: *ivfflat should never be created on empty tables* because it needs to cluster data, and that only happens when an index is first created, not when new rows are inserted or modified. Also, if your table undergoes a lot of modifications, you will need to rebuild this index occasionally to maintain good accuracy. See our [blog post](https://www.timescale.com/blog/nearest-neighbor-indexes-what-are-ivfflat-indexes-in-pgvector-and-how-do-they-work/) for details.\n",
    "\n",
    "Pgvector ivfflat has a `lists` index parameter that is automatically set with a smart default based on the number of rows in your table. If you know that you'll have a different table size, you can specify the number of records to use for calculating the `lists` parameter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vec.drop_embedding_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.IvfflatIndex(num_records=1000000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set the `lists` parameter directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vec.drop_embedding_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.create_embedding_index(client.IvfflatIndex(num_lists=100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set a parameter to control the accuracy vs. query speed trade-off at query time. The parameter is set in the `search()` function using the `query_params` argument. You can set the `probes`. This parameter specifies the number of clusters searched during a query. It is recommended to set this parameter to `sqrt(lists)` where lists is the `num_list` parameter used above during index creation. Higher values improve query accuracy while making the query slower.\n",
    "\n",
    "You can specify this value during search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec.search([1.0, 9.0], limit=4, query_params=IvfflatIndexParams(probes=10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop the index, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.drop_embedding_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time partitioning\n",
    "\n",
    "In many use cases where you have many embeddings, time is an important component associated with the embeddings. For example, when embedding news stories, you often search by time as well as similarity (e.g., stories related to Bitcoin in the past week or stories about Clinton in November 2016). \n",
    "\n",
    "Yet, traditionally, searching by two components \"similarity\" and \"time\" is challenging for Approximate Nearest Neighbor (ANN) indexes and makes the similarity-search index less effective.\n",
    "\n",
    "One approach to solving this is partitioning the data by time and creating ANN indexes on each partition individually. Then, during search, you can:\n",
    "\n",
    " * Step 1: filter our partitions that don't match the time predicate.\n",
    " * Step 2: perform the similarity search on all matching partitions.\n",
    " * Step 3: combine all the results from each partition in step 2, rerank, and filter out results by time.\n",
    "\n",
    "Step 1 makes the search a lot more efficient by filtering out whole swaths of data in one go.\n",
    "\n",
    "Timescale-vector supports time partitioning using TimescaleDB's hypertables. To use this feature, simply indicate the length of time for each partition when creating the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = client.Async(service_url, \"my_data_with_time_partition\", 2, time_partition_interval=timedelta(hours=6))\n",
    "await vec.create_tables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, insert data where the IDs use UUIDs v1 and the time component of the UUID specifies the time of the embedding.\n",
    "For example, to create an embedding for the current time, simply do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = uuid.uuid1()\n",
    "await vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert data for a specific time in the past, create the UUID using our `uuid_from_time` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_datetime = datetime(2018, 8, 10, 15, 30, 0)\n",
    "await vec.upsert([(client.uuid_from_time(specific_datetime), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then query the data by specifying a `uuid_time_filter` in the search call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=client.UUIDTimeRange(specific_datetime-timedelta(days=7), specific_datetime+timedelta(days=7)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance metrics\n",
    "\n",
    "By default, we use cosine distance to measure how similarly an embedding is to a given query. In addition to cosine distance, we also support Euclidean/L2 distance. The distance type is set when creating the client using the `distance_type` parameter. For example, to use the Euclidean distance metric, you can create the client with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec  = client.Sync(service_url, \"my_data\", 2, distance_type=\"euclidean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid values for `distance_type` are `cosine` and `euclidean`.\n",
    "\n",
    "It is important to note that you should use consistent distance types on clients that create indexes and perform queries. That is because an index is only valid for one particular type of distance measure.\n",
    "\n",
    "Please note the Timescale Vector index only supports cosine distance at this time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain integration\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a popular framework for development applications powered by LLMs. Timescale Vector has a native LangChain integration, enabling you to use Timescale Vector as a vectorstore and leverage all its capabilities in your applications built with LangChain.\n",
    "\n",
    "Here are resources about using Timescale Vector with LangChain:\n",
    "\n",
    "- [Getting started with LangChain and Timescale Vector](https://python.langchain.com/docs/integrations/vectorstores/timescalevector): You'll learn how to use Timescale Vector for (1) semantic search, (2) time-based vector search, (3) self-querying, and (4) how to create indexes to speed up queries.\n",
    "- [PostgreSQL Self Querying](https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query): Learn how to use Timescale Vector with self-querying in LangChain. \n",
    "- [LangChain template: RAG with conversational retrieval](https://github.com/langchain-ai/langchain/tree/master/templates/rag-timescale-conversation): This template is used for conversational retrieval, which is one of the most popular LLM use-cases. It passes both a conversation history and retrieved documents into an LLM for synthesis.\n",
    "- [LangChain template: RAG with time-based search and self-query retrieval](https://github.com/langchain-ai/langchain/tree/master/templates/rag-timescale-hybrid-search-time):This template shows how to use timescale-vector with the self-query retriver to perform hybrid search on similarity and time. This is useful any time your data has a strong time-based component. \n",
    "- [Learn more about Timescale Vector and LangChain](https://blog.langchain.dev/timescale-vector-x-langchain-making-postgresql-a-better-vector-database-for-ai-applications/)\n",
    "\n",
    "# LlamaIndex integration\n",
    "\n",
    "[LlamaIndex] is a popular data framework for connecting custom data sources to large language models (LLMs). Timescale Vector has a native LlamaIndex integration, enabling you to use Timescale Vector as a vectorstore and leverage all its capabilities in your applications built with LlamaIndex.\n",
    "\n",
    "Here are resources about using Timescale Vector with LlamaIndex:\n",
    "\n",
    "- [Getting started with LlamaIndex and Timescale Vector](https://docs.llamaindex.ai/en/stable/examples/vector_stores/Timescalevector.html): You’ll learn how to use Timescale Vector for (1) similarity search, (2) time-based vector search, (3) faster search with indexes, and (4) retrieval and query engine. \n",
    "- [Time-based retrieval](https://youtu.be/EYMZVfKcRzM?si=I0H3uUPgzKbQw__W): Learn how to power RAG applications with time-based retrieval.\n",
    "- [Llama Pack: Auto Retrieval with time-based search](https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs/timescale_vector_autoretrieval): This pack demonstrates performing auto-retrieval for hybrid search based on both similarity and time, using the timescale-vector (PostgreSQL) vectorstore.  \n",
    "- [Learn more about Timescale Vector and LlamaIndex ](https://www.timescale.com/blog/timescale-vector-x-llamaindex-making-postgresql-a-better-vector-database-for-ai-applications/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PgVectorize\n",
    "\n",
    "PgVectorize enables you to create vector embeddings from any data that\n",
    "you already have stored in PostgreSQL. You can get more background information in our [blog post](https://www.timescale.com/blog/a-complete-guide-to-creating-and-storing-embeddings-for-postgresql-data/) announcing this feature, as well as a [\"how we built in\"](https://www.timescale.com/blog/how-we-designed-a-resilient-vector-embedding-creation-system-for-postgresql-data/) post going into the details of the design.\n",
    "\n",
    "To create vector embeddings, simply attach PgVectorize to any PostgreSQL table, and it will automatically sync that table's data with a set of embeddings stored in Timescale Vector. For example, let's say you have a blog table defined in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from timescale_vector import client, pgvectorizer\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.timescalevector import TimescaleVector\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for item in ['blog', 'blog_embedding_work_queue', 'blog_embedding']:\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {item};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS blog (\n",
    "            id              SERIAL PRIMARY KEY NOT NULL,\n",
    "            title           TEXT NOT NULL,\n",
    "            author          TEXT NOT NULL,\n",
    "            contents        TEXT NOT NULL,\n",
    "            category        TEXT NOT NULL,\n",
    "            published_time  TIMESTAMPTZ NULL --NULL if not yet published\n",
    "        );\n",
    "        ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert some data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(service_url) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO blog (title, author, contents, category, published_time) VALUES ('First Post', 'Matvey Arye', 'some super interesting content about cats.', 'AI', '2021-01-01');\n",
    "        ''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, say you want to embed these blogs in Timescale Vector. First, you need to define an `embed_and_write` function that takes a set of blog posts, creates the embeddings, and writes them into TimescaleVector. For example, if using LangChain, it could look something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(blog):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    docs = []\n",
    "    for chunk in text_splitter.split_text(blog['contents']):\n",
    "        content = f\"Author {blog['author']}, title: {blog['title']}, contents:{chunk}\"\n",
    "        metadata = {\n",
    "            \"id\": str(client.uuid_from_time(blog['published_time'])),\n",
    "            \"blog_id\": blog['id'], \n",
    "            \"author\": blog['author'], \n",
    "            \"category\": blog['category'],\n",
    "            \"published_time\": blog['published_time'].isoformat(),\n",
    "        }\n",
    "        docs.append(Document(page_content=content, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def embed_and_write(blog_instances, vectorizer):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vector_store = TimescaleVector(\n",
    "        collection_name=\"blog_embedding\",\n",
    "        service_url=service_url,\n",
    "        embedding=embedding,\n",
    "        time_partition_interval=timedelta(days=30),\n",
    "    )\n",
    "\n",
    "    # delete old embeddings for all ids in the work queue. locked_id is a special column that is set to the primary key of the table being\n",
    "    # embedded. For items that are deleted, it is the only key that is set.\n",
    "    metadata_for_delete = [{\"blog_id\": blog['locked_id']} for blog in blog_instances]\n",
    "    vector_store.delete_by_metadata(metadata_for_delete)\n",
    "\n",
    "    documents = []\n",
    "    for blog in blog_instances:\n",
    "        # skip blogs that are not published yet, or are deleted (in which case it will be NULL)\n",
    "        if blog['published_time'] != None:\n",
    "            documents.extend(get_document(blog))\n",
    "\n",
    "    if len(documents) == 0:\n",
    "        return\n",
    "    \n",
    "    texts = [d.page_content for d in documents]\n",
    "    metadatas = [d.metadata for d in documents]\n",
    "    ids = [d.metadata[\"id\"] for d in documents]\n",
    "    vector_store.add_texts(texts, metadatas, ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all you have to do is run the following code in a scheduled job (cron job, Lambda job, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this job should be run on a schedule\n",
    "vectorizer = pgvectorizer.Vectorize(service_url, 'blog')\n",
    "while vectorizer.process(embed_and_write) > 0:\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time that job runs, it will sync the table with your embeddings. It will sync all inserts, updates, and deletes to an embeddings table called `blog_embedding`.\n",
    "\n",
    "Now, you can simply search the embeddings as follows (again, using LangChain in the example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Author Matvey Arye, title: First Post, contents:some super interesting content about cats.', metadata={'id': '4a784000-4bc4-11eb-855a-06302dbc8ce7', 'author': 'Matvey Arye', 'blog_id': 1, 'category': 'AI', 'published_time': '2021-01-01T00:00:00+00:00'}),\n",
       "  0.12595687795193833)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vector_store = TimescaleVector(\n",
    "    collection_name=\"blog_embedding\",\n",
    "    service_url=service_url,\n",
    "    embedding=embedding,\n",
    "    time_partition_interval=timedelta(days=30),\n",
    ")\n",
    "\n",
    "res = vector_store.similarity_search_with_score(\"Blogs about cats\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "This project is developed with [nbdev](https://nbdev.fast.ai/). Please see that website for the development process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
