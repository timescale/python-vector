{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client\n",
    "\n",
    "> A module for writing and querying vectors to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv(), override=True)\n",
    "service_url = os.environ['TIMESCALE_SERVICE_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncpg\n",
    "import uuid\n",
    "from pgvector.asyncpg import register_vector\n",
    "from typing import (List, Optional, Union, Dict, Tuple, Any, Iterable, Callable)\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#copied from Cassandra: https://docs.datastax.com/en/drivers/python/3.2/_modules/cassandra/util.html#uuid_from_time\n",
    "def uuid_from_time(time_arg = None, node=None, clock_seq=None):\n",
    "    \"\"\"\n",
    "    Converts a datetime or timestamp to a type 1 `uuid.UUID`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_arg\n",
    "        The time to use for the timestamp portion of the UUID.\n",
    "        This can either be a `datetime` object or a timestamp in seconds\n",
    "        (as returned from `time.time()`).\n",
    "    node\n",
    "        Bytes for the UUID (up to 48 bits). If not specified, this\n",
    "        field is randomized.\n",
    "    clock_seq\n",
    "        Clock sequence field for the UUID (up to 14 bits). If not specified,\n",
    "        a random sequence is generated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        uuid.UUID:  For the given time, node, and clock sequence\n",
    "    \"\"\"\n",
    "    if time_arg is None:\n",
    "        return uuid.uuid1(node, clock_seq)\n",
    "    if hasattr(time_arg, 'utctimetuple'):\n",
    "        # this is different from the Cassandra version, we assume that a naive datetime is in system time and convert it to UTC\n",
    "        # we do this because naive datetimes are interpreted as timestamps (without timezone) in postgres\n",
    "        if time_arg.tzinfo is None:\n",
    "            time_arg = time_arg.astimezone(timezone.utc)\n",
    "        seconds = int(calendar.timegm(time_arg.utctimetuple()))\n",
    "        microseconds = (seconds * 1e6) + time_arg.time().microsecond\n",
    "    else:\n",
    "        microseconds = int(time_arg * 1e6)\n",
    "\n",
    "    # 0x01b21dd213814000 is the number of 100-ns intervals between the\n",
    "    # UUID epoch 1582-10-15 00:00:00 and the Unix epoch 1970-01-01 00:00:00.\n",
    "    intervals = int(microseconds * 10) + 0x01b21dd213814000\n",
    "\n",
    "    time_low = intervals & 0xffffffff\n",
    "    time_mid = (intervals >> 32) & 0xffff\n",
    "    time_hi_version = (intervals >> 48) & 0x0fff\n",
    "\n",
    "    if clock_seq is None:\n",
    "        clock_seq = random.getrandbits(14)\n",
    "    else:\n",
    "        if clock_seq > 0x3fff:\n",
    "            raise ValueError('clock_seq is out of range (need a 14-bit value)')\n",
    "\n",
    "    clock_seq_low = clock_seq & 0xff\n",
    "    clock_seq_hi_variant = 0x80 | ((clock_seq >> 8) & 0x3f)\n",
    "\n",
    "    if node is None:\n",
    "        node = random.getrandbits(48)\n",
    "\n",
    "    return uuid.UUID(fields=(time_low, time_mid, time_hi_version,\n",
    "                             clock_seq_hi_variant, clock_seq_low, node), version=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseIndex:\n",
    "    def get_index_method(self, distance_type: str) -> str:\n",
    "        index_method = \"invalid\"\n",
    "        if distance_type == \"<->\":\n",
    "            index_method = \"vector_l2_ops\"\n",
    "        elif distance_type == \"<#>\":\n",
    "            index_method = \"vector_ip_ops\"\n",
    "        elif distance_type == \"<=>\":\n",
    "            index_method = \"vector_cosine_ops\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance type {distance_type}\")\n",
    "        return index_method\n",
    "\n",
    "    def create_index_query(self, table_name_quoted:str, column_name_quoted: str, index_name_quoted: str, distance_type: str, num_records_callback: Callable[[], int]) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class IvfflatIndex(BaseIndex):\n",
    "    def __init__(self, num_records: Optional[int] = None, num_lists: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Pgvector's ivfflat index.\n",
    "        \"\"\"\n",
    "        self.num_records = num_records\n",
    "        self.num_lists = num_lists\n",
    "    \n",
    "    def get_num_records(self, num_record_callback: Callable[[], int]) -> int:\n",
    "        if self.num_records is not None:\n",
    "            return self.num_records\n",
    "        return num_record_callback()\n",
    "\n",
    "    def get_num_lists(self, num_records_callback: Callable[[], int]) -> int:\n",
    "        if self.num_lists is not None:\n",
    "            return self.num_lists\n",
    "        \n",
    "        num_records = self.get_num_records(num_records_callback)\n",
    "        num_lists = num_records / 1000\n",
    "        if num_lists < 10:\n",
    "            num_lists = 10\n",
    "        if num_records > 1000000:\n",
    "            num_lists = math.sqrt(num_records)\n",
    "        return num_lists\n",
    "    \n",
    "\n",
    "    def create_index_query(self, table_name_quoted:str, column_name_quoted: str, index_name_quoted: str, distance_type: str, num_records_callback: Callable[[], int]) -> str:\n",
    "        index_method = self.get_index_method(distance_type)\n",
    "        num_lists = self.get_num_lists(num_records_callback)\n",
    "\n",
    "        return \"CREATE INDEX {index_name} ON {table_name} USING ivfflat ({column_name} {index_method}) WITH (lists = {num_lists});\"\\\n",
    "            .format(index_name=index_name_quoted, table_name=table_name_quoted, column_name=column_name_quoted, index_method=index_method, num_lists=num_lists)\n",
    "\n",
    "\n",
    "class HNSWIndex(BaseIndex):\n",
    "    def __init__(self, m: Optional[int] = None, ef_construction: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Pgvector's hnsw index.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.ef_construction = ef_construction\n",
    "\n",
    "    def create_index_query(self, table_name_quoted:str, column_name_quoted: str, index_name_quoted: str, distance_type: str, num_records_callback: Callable[[], int]) -> str:\n",
    "        index_method = self.get_index_method(distance_type)\n",
    "\n",
    "        with_clauses = []\n",
    "        if self.m is not None:\n",
    "            with_clauses.append(f\"m = {self.m}\")\n",
    "        if self.ef_construction is not None:\n",
    "            with_clauses.append(f\"ef_construction = {self.ef_construction}\")\n",
    "        \n",
    "        with_clause = \"\"\n",
    "        if len(with_clauses) > 0:\n",
    "            with_clause = \"WITH (\" + \", \".join(with_clauses) + \")\"\n",
    "\n",
    "        return \"CREATE INDEX {index_name} ON {table_name} USING hnsw ({column_name} {index_method}) {with_clause};\"\\\n",
    "            .format(index_name=index_name_quoted, table_name=table_name_quoted, column_name=column_name_quoted, index_method=index_method, with_clause=with_clause)\n",
    "\n",
    "class TimescaleVectorIndex(BaseIndex):\n",
    "    def __init__(self, \n",
    "                 use_pq: Optional[bool] = None, \n",
    "                 num_neighbors: Optional[int] = None, \n",
    "                 search_list_size: Optional[int] = None, \n",
    "                 max_alpha: Optional[float] = None,\n",
    "                 pq_vector_length: Optional[int] = None,\n",
    "                 ) -> None:\n",
    "        \"\"\"\n",
    "        Timescale's vector index.\n",
    "        \"\"\"\n",
    "        self.use_pq = use_pq\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.search_list_size = search_list_size\n",
    "        self.max_alpha = max_alpha\n",
    "        self.pq_vector_length = pq_vector_length\n",
    "\n",
    "    def create_index_query(self, table_name_quoted:str, column_name_quoted: str, index_name_quoted: str, distance_type: str, num_records_callback: Callable[[], int]) -> str:\n",
    "\n",
    "        with_clauses = []\n",
    "        if self.use_pq is not None:\n",
    "            with_clauses.append(f\"use_pq = {self.use_pq}\")\n",
    "        if self.num_neighbors is not None:\n",
    "            with_clauses.append(f\"num_neighbors = {self.num_neighbors}\")\n",
    "        if self.search_list_size is not None:\n",
    "            with_clauses.append(f\"search_list_size = {self.search_list_size}\")\n",
    "        if self.max_alpha is not None:\n",
    "            with_clauses.append(f\"max_alpha = {self.max_alpha}\")\n",
    "        if self.pq_vector_length is not None:\n",
    "            with_clauses.append(f\"pq_vector_length = {self.pq_vector_length}\")\n",
    "        \n",
    "        with_clause = \"\"\n",
    "        if len(with_clauses) > 0:\n",
    "            with_clause = \"WITH (\" + \", \".join(with_clauses) + \")\"\n",
    "\n",
    "        return \"CREATE INDEX {index_name} ON {table_name} USING tsv ({column_name}) {with_clause};\"\\\n",
    "            .format(index_name=index_name_quoted, table_name=table_name_quoted, column_name=column_name_quoted, with_clause=with_clause)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "SEARCH_RESULT_ID_IDX = 0\n",
    "SEARCH_RESULT_METADATA_IDX = 1\n",
    "SEARCH_RESULT_CONTENTS_IDX = 2\n",
    "SEARCH_RESULT_EMBEDDING_IDX = 3\n",
    "SEARCH_RESULT_DISTANCE_IDX = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UUIDTimeRange:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_datetime(input_datetime: Union[datetime, str]):\n",
    "        \"\"\"\n",
    "        Parse a datetime object or string representation of a datetime.\n",
    "\n",
    "        Args:\n",
    "            input_datetime (datetime or str): Input datetime or string.\n",
    "\n",
    "        Returns:\n",
    "            datetime: Parsed datetime object.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input cannot be parsed as a datetime.\n",
    "        \"\"\"\n",
    "        if input_datetime is None or input_datetime == \"None\":\n",
    "            return None\n",
    "        \n",
    "        if isinstance(input_datetime, datetime):\n",
    "            # If input is already a datetime object, return it as is\n",
    "            return input_datetime\n",
    "\n",
    "        if isinstance(input_datetime, str):\n",
    "            try:\n",
    "                # Attempt to parse the input string into a datetime\n",
    "                return datetime.fromisoformat(input_datetime)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Invalid datetime string format: {}\".format(input_datetime))\n",
    "\n",
    "        raise ValueError(\"Input must be a datetime object or string\")\n",
    "\n",
    "    def __init__(self, start_date: Optional[Union[datetime, str]] = None, end_date: Optional[Union[datetime, str]] = None, time_delta: Optional[timedelta] = None, start_inclusive=True, end_inclusive=False):\n",
    "        \"\"\"\n",
    "         A UUIDTimeRange is a time range predicate on the UUID Version 1 timestamps. \n",
    "         \n",
    "         Note that naive datetime objects are interpreted as local time on the python client side and converted to UTC before being sent to the database.\n",
    "        \"\"\"\n",
    "        start_date = UUIDTimeRange._parse_datetime(start_date)\n",
    "        end_date = UUIDTimeRange._parse_datetime(end_date)\n",
    "\n",
    "        if start_date is not None and end_date is not None:\n",
    "            if start_date > end_date:\n",
    "                raise Exception(\"start_date must be before end_date\")\n",
    "        \n",
    "        if start_date is None and end_date is None:\n",
    "            raise Exception(\"start_date and end_date cannot both be None\")\n",
    "        \n",
    "        if start_date is not None and start_date.tzinfo is None:\n",
    "            start_date = start_date.astimezone(timezone.utc)\n",
    "\n",
    "        if end_date is not None and end_date.tzinfo is None:\n",
    "            end_date = end_date.astimezone(timezone.utc)\n",
    "        \n",
    "        if time_delta is not None:\n",
    "            if end_date is None:\n",
    "                end_date = start_date + time_delta\n",
    "            elif start_date is None:\n",
    "                start_date = end_date - time_delta\n",
    "            else:\n",
    "                raise Exception(\"time_delta, start_date and end_date cannot all be specified at the same time\")\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.start_inclusive = start_inclusive\n",
    "        self.end_inclusive = end_inclusive\n",
    "    \n",
    "    def __str__(self):\n",
    "        start_str = f\"[{self.start_date}\" if self.start_inclusive else f\"({self.start_date}\"\n",
    "        end_str = f\"{self.end_date}]\" if self.end_inclusive else f\"{self.end_date})\"\n",
    "        \n",
    "        return f\"UUIDTimeRange {start_str}, {end_str}\"\n",
    "\n",
    "    def build_query(self, params: List) -> Tuple[str, List]:\n",
    "        column = \"uuid_timestamp(id)\"\n",
    "        queries = []\n",
    "        if self.start_date is not None:\n",
    "            if self.start_inclusive:\n",
    "                queries.append(f\"{column} >= ${len(params)+1}\")\n",
    "            else:\n",
    "                queries.append(f\"{column} > ${len(params)+1}\")\n",
    "            params.append(self.start_date)\n",
    "        if self.end_date is not None:\n",
    "            if self.end_inclusive:\n",
    "                queries.append(f\"{column} <= ${len(params)+1}\")\n",
    "            else:\n",
    "                queries.append(f\"{column} < ${len(params)+1}\")\n",
    "            params.append(self.end_date)\n",
    "        return \" AND \".join(queries), params         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Predicates:\n",
    "    logical_operators = {\n",
    "        \"AND\": \"AND\",\n",
    "        \"OR\": \"OR\",\n",
    "        \"NOT\": \"NOT\",\n",
    "    }\n",
    "\n",
    "    operators_mapping = {\n",
    "        \"=\": \"=\",\n",
    "        \"==\": \"=\",\n",
    "        \">=\": \">=\",\n",
    "        \">\": \">\",\n",
    "        \"<=\": \"<=\",\n",
    "        \"<\": \"<\",\n",
    "        \"!=\": \"<>\",\n",
    "    }\n",
    "\n",
    "    PredicateValue = Union[str, int, float]\n",
    "\n",
    "    def __init__(self, *clauses: Union['Predicates', Tuple[str, PredicateValue], Tuple[str, str, PredicateValue], str, PredicateValue], operator: str = 'AND'):\n",
    "        \"\"\"\n",
    "        Predicates class defines predicates on the object metadata. Predicates can be combined using logical operators (&, |, and ~).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clauses\n",
    "            Predicate clauses. Can be either another Predicates object or a tuple of the form (field, operator, value) or (field, value).\n",
    "        Operator\n",
    "            Logical operator to use when combining the clauses. Can be one of 'AND', 'OR', 'NOT'. Defaults to 'AND'.\n",
    "        \"\"\"\n",
    "        if operator not in self.logical_operators: \n",
    "            raise ValueError(f\"invalid operator: {operator}\")\n",
    "        self.operator = operator\n",
    "        if isinstance(clauses[0], str):\n",
    "            if len(clauses) != 3 or not (isinstance(clauses[1], str) and isinstance(clauses[2], self.PredicateValue)):\n",
    "                raise ValueError(f\"Invalid clause format: {clauses}\")\n",
    "            self.clauses = [(clauses[0], clauses[1], clauses[2])]\n",
    "        else:\n",
    "            self.clauses = list(clauses)\n",
    "\n",
    "    def add_clause(self, *clause: Union['Predicates', Tuple[str, PredicateValue], Tuple[str, str, PredicateValue], str, PredicateValue]):\n",
    "        \"\"\"\n",
    "        Add a clause to the predicates object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clause: 'Predicates' or Tuple[str, str] or Tuple[str, str, str]\n",
    "            Predicate clause. Can be either another Predicates object or a tuple of the form (field, operator, value) or (field, value).\n",
    "        \"\"\"\n",
    "        if isinstance(clause[0], str):\n",
    "            if len(clause) != 3 or not (isinstance(clause[1], str) and isinstance(clause[2], self.PredicateValue)):\n",
    "                raise ValueError(f\"Invalid clause format: {clause}\")\n",
    "            self.clauses.append((clause[0], clause[1], clause[2]))\n",
    "        else:\n",
    "            self.clauses.extend(list(clause))\n",
    "        \n",
    "    def __and__(self, other):\n",
    "        new_predicates = Predicates(self, other, operator='AND')\n",
    "        return new_predicates\n",
    "\n",
    "    def __or__(self, other):\n",
    "        new_predicates = Predicates(self, other, operator='OR')\n",
    "        return new_predicates\n",
    "\n",
    "    def __invert__(self):\n",
    "        new_predicates = Predicates(self, operator='NOT')\n",
    "        return new_predicates\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Predicates):\n",
    "            return False\n",
    "\n",
    "        return (\n",
    "            self.operator == other.operator and\n",
    "            self.clauses == other.clauses\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.operator:\n",
    "            return f\"{self.operator}({', '.join(repr(clause) for clause in self.clauses)})\"\n",
    "        else:\n",
    "            return repr(self.clauses)\n",
    "\n",
    "    def build_query(self, params: List) -> Tuple[str, List]:\n",
    "        \"\"\"\n",
    "        Build the SQL query string and parameters for the predicates object.\n",
    "        \"\"\"\n",
    "        if not self.clauses:\n",
    "            return \"\", []\n",
    "\n",
    "        where_conditions = [] \n",
    "\n",
    "        for clause in self.clauses:\n",
    "            if isinstance(clause, Predicates):\n",
    "                child_where_clause, params = clause.build_query(params)\n",
    "                where_conditions.append(f\"({child_where_clause})\")\n",
    "            elif isinstance(clause, tuple):\n",
    "                if len(clause) == 2:\n",
    "                    field, value = clause\n",
    "                    operator = \"=\"  # Default operator\n",
    "                elif len(clause) == 3:\n",
    "                    field, operator, value = clause\n",
    "                    if operator not in self.operators_mapping:\n",
    "                       raise ValueError(f\"Invalid operator: {operator}\") \n",
    "                    operator = self.operators_mapping[operator]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid clause format\")\n",
    "\n",
    "                index = len(params)+1\n",
    "                param_name = f\"${index}\"\n",
    "\n",
    "                if field == '__uuid_timestamp':\n",
    "                    #convert str to timestamp in the database, it's better at it than python\n",
    "                    if isinstance(value, str):\n",
    "                        where_conditions.append(f\"uuid_timestamp(id) {operator} ({param_name}::text)::timestamptz\")\n",
    "                    else:\n",
    "                        where_conditions.append(f\"uuid_timestamp(id) {operator} {param_name}\")\n",
    "                    params.append(value)\n",
    "                    continue\n",
    "\n",
    "                field_cast = ''\n",
    "                if isinstance(value, int):\n",
    "                    field_cast = '::int'\n",
    "                elif isinstance(value, float):\n",
    "                    field_cast = '::numeric'  \n",
    "\n",
    "                where_conditions.append(f\"(metadata->>'{field}'){field_cast} {operator} {param_name}\")\n",
    "                params.append(value) \n",
    "\n",
    "        if self.operator == 'NOT':\n",
    "            or_clauses = (\" OR \").join(where_conditions)\n",
    "            #use IS DISTINCT FROM to treat all-null clauses as False and pass the filter\n",
    "            where_clause = f\"TRUE IS DISTINCT FROM ({or_clauses})\"\n",
    "        else:\n",
    "            where_clause = (\" \"+self.operator+\" \").join(where_conditions)\n",
    "        return where_clause, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QueryBuilder:\n",
    "    def __init__(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            num_dimensions: int,\n",
    "            distance_type: str,\n",
    "            id_type: str,\n",
    "            time_partition_interval: Optional[timedelta],\n",
    "            infer_filters: bool) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a base Vector object to generate queries for vector clients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        table_name\n",
    "            The name of the table.\n",
    "        num_dimensions\n",
    "            The number of dimensions for the embedding vector.\n",
    "        distance_type\n",
    "            The distance type for indexing.\n",
    "        id_type\n",
    "            The type of the id column. Can be either 'UUID' or 'TEXT'.\n",
    "        \"\"\"\n",
    "        self.table_name = table_name\n",
    "        self.num_dimensions = num_dimensions\n",
    "        if distance_type == 'cosine' or distance_type == '<=>':\n",
    "            self.distance_type = '<=>'\n",
    "        elif distance_type == 'euclidean' or distance_type == '<->' or distance_type == 'l2':\n",
    "            self.distance_type = '<->'\n",
    "        else:\n",
    "            raise ValueError(f\"unrecognized distance_type {distance_type}\")\n",
    "\n",
    "        if id_type.lower() != 'uuid' and id_type.lower() != 'text':\n",
    "            raise ValueError(f\"unrecognized id_type {id_type}\")\n",
    "\n",
    "        if time_partition_interval is not None and id_type.lower() != 'uuid':\n",
    "            raise ValueError(f\"time partitioning is only supported for uuid id_type\")\n",
    "\n",
    "        self.id_type = id_type.lower()\n",
    "        self.time_partition_interval = time_partition_interval\n",
    "        self.infer_filters = infer_filters\n",
    "\n",
    "    @staticmethod\n",
    "    def _quote_ident(ident):\n",
    "        \"\"\"\n",
    "        Quotes an identifier to prevent SQL injection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ident\n",
    "            The identifier to be quoted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The quoted identifier.\n",
    "        \"\"\"\n",
    "        return '\"{}\"'.format(ident.replace('\"', '\"\"'))\n",
    "\n",
    "    def get_row_exists_query(self):\n",
    "        \"\"\"\n",
    "        Generates a query to check if any rows exist in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The query to check for row existence.\n",
    "        \"\"\"\n",
    "        return \"SELECT 1 FROM {table_name} LIMIT 1\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    def get_upsert_query(self):\n",
    "        \"\"\"\n",
    "        Generates an upsert query.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The upsert query.\n",
    "        \"\"\"\n",
    "        return \"INSERT INTO {table_name} (id, metadata, contents, embedding) VALUES ($1, $2, $3, $4) ON CONFLICT DO NOTHING\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    def get_approx_count_query(self):\n",
    "        \"\"\"\n",
    "        Generate a query to find the approximate count of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: the query.\n",
    "        \"\"\"\n",
    "        # todo optimize with approx\n",
    "        return \"SELECT COUNT(*) as cnt FROM {table_name}\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    #| export\n",
    "    def get_create_query(self):\n",
    "        \"\"\"\n",
    "        Generates a query to create the tables, indexes, and extensions needed to store the vector data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The create table query.\n",
    "        \"\"\"\n",
    "        hypertable_sql = \"\"\n",
    "        if self.time_partition_interval is not None:\n",
    "            hypertable_sql = '''\n",
    "                CREATE EXTENSION IF NOT EXISTS timescaledb;\n",
    "\n",
    "                CREATE OR REPLACE FUNCTION public.uuid_timestamp(uuid UUID) RETURNS TIMESTAMPTZ AS $$\n",
    "                DECLARE\n",
    "                bytes bytea;\n",
    "                BEGIN\n",
    "                bytes := uuid_send(uuid);\n",
    "                if  (get_byte(bytes, 6) >> 4)::int2 != 1 then\n",
    "                    RAISE EXCEPTION 'UUID version is not 1';\n",
    "                end if;\n",
    "                RETURN to_timestamp(\n",
    "                            (\n",
    "                                (\n",
    "                                (get_byte(bytes, 0)::bigint << 24) |\n",
    "                                (get_byte(bytes, 1)::bigint << 16) |\n",
    "                                (get_byte(bytes, 2)::bigint <<  8) |\n",
    "                                (get_byte(bytes, 3)::bigint <<  0)\n",
    "                                ) + (\n",
    "                                ((get_byte(bytes, 4)::bigint << 8 |\n",
    "                                get_byte(bytes, 5)::bigint)) << 32\n",
    "                                ) + (\n",
    "                                (((get_byte(bytes, 6)::bigint & 15) << 8 | get_byte(bytes, 7)::bigint) & 4095) << 48\n",
    "                                ) - 122192928000000000\n",
    "                            ) / 10000 / 1000::double precision\n",
    "                        );\n",
    "                END\n",
    "                $$ LANGUAGE plpgsql\n",
    "                IMMUTABLE PARALLEL SAFE\n",
    "                RETURNS NULL ON NULL INPUT;\n",
    "\n",
    "                SELECT create_hypertable('{table_name}', \n",
    "                    'id', \n",
    "                    if_not_exists=> true, \n",
    "                    time_partitioning_func=>'public.uuid_timestamp', \n",
    "                    chunk_time_interval => '{chunk_time_interval} seconds'::interval);\n",
    "            '''.format(\n",
    "                table_name=self._quote_ident(self.table_name), \n",
    "                chunk_time_interval=str(self.time_partition_interval.total_seconds()),\n",
    "                )\n",
    "        return '''\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "CREATE EXTENSION IF NOT EXISTS timescale_vector;\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id {id_type} PRIMARY KEY,\n",
    "    metadata JSONB,\n",
    "    contents TEXT,\n",
    "    embedding VECTOR({dimensions})\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} USING GIN(metadata jsonb_path_ops);\n",
    "\n",
    "{hypertable_sql}\n",
    "'''.format(\n",
    "            table_name=self._quote_ident(self.table_name), \n",
    "            id_type=self.id_type, \n",
    "            index_name=self._quote_ident(self.table_name+\"_meta_idx\"), \n",
    "            dimensions=self.num_dimensions,\n",
    "            hypertable_sql=hypertable_sql,\n",
    "            )\n",
    "\n",
    "    def _get_embedding_index_name(self):\n",
    "        return self._quote_ident(self.table_name+\"_embedding_idx\")\n",
    "\n",
    "    def drop_embedding_index_query(self):\n",
    "        return \"DROP INDEX IF EXISTS {index_name};\".format(index_name=self._get_embedding_index_name())\n",
    "\n",
    "    def delete_all_query(self):\n",
    "        return \"TRUNCATE {table_name};\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    def delete_by_ids_query(self, ids: Union[List[uuid.UUID], List[str]]) -> Tuple[str, List]:\n",
    "        query = \"DELETE FROM {table_name} WHERE id = ANY($1::{id_type}[]);\".format(\n",
    "            table_name=self._quote_ident(self.table_name), id_type=self.id_type)\n",
    "        return (query, [ids])\n",
    "\n",
    "    def delete_by_metadata_query(self, filter: Union[Dict[str, str], List[Dict[str, str]]]) -> Tuple[str, List]:\n",
    "        params: List[Any] = []\n",
    "        (where, params) = self._where_clause_for_filter(params, filter)\n",
    "        query = \"DELETE FROM {table_name} WHERE {where};\".format(\n",
    "            table_name=self._quote_ident(self.table_name), where=where)\n",
    "        return (query, params)\n",
    "\n",
    "    def drop_table_query(self):\n",
    "        return \"DROP TABLE IF EXISTS {table_name};\".format(table_name=self._quote_ident(self.table_name))\n",
    "    \n",
    "    def default_max_db_connection_query(self):\n",
    "        \"\"\"\n",
    "        Generates a query to get the default max db connections. This uses a heuristic to determine the max connections based on the max_connections setting in postgres\n",
    "        and the number of currently used connections. This heuristic leaves 4 connections in reserve.\n",
    "        \"\"\"\n",
    "        return \"SELECT greatest(1, ((SELECT setting::int FROM pg_settings WHERE name='max_connections')-(SELECT count(*) FROM pg_stat_activity) - 4)::int)\"\n",
    "    \n",
    "    def create_embedding_index_query(self, index: BaseIndex, num_records_callback: Callable[[], int]) -> str:\n",
    "        \"\"\"\n",
    "        Generates an embedding index creation query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "            The index to create.\n",
    "        num_records_callback\n",
    "            A callback function to get the number of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The index creation query.\n",
    "        \"\"\"\n",
    "        column_name = \"embedding\"\n",
    "        index_name = self._get_embedding_index_name()\n",
    "        query = index.create_index_query(self._quote_ident(self.table_name), self._quote_ident(column_name), index_name, self.distance_type, num_records_callback)\n",
    "        return query\n",
    "\n",
    "    def _where_clause_for_filter(self, params: List, filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]]) -> Tuple[str, List]:\n",
    "        if filter == None:\n",
    "            return (\"TRUE\", params)\n",
    "\n",
    "        if isinstance(filter, dict):\n",
    "            where = \"metadata @> ${index}\".format(index=len(params)+1)\n",
    "            json_object = json.dumps(filter)\n",
    "            params = params + [json_object]\n",
    "        elif isinstance(filter, list):\n",
    "            any_params = []\n",
    "            for idx, filter_dict in enumerate(filter, start=len(params) + 1):\n",
    "                any_params.append(json.dumps(filter_dict))\n",
    "            where = \"metadata @> ANY(${index}::jsonb[])\".format(\n",
    "                index=len(params) + 1)\n",
    "            params = params + [any_params]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown filter type: {filter_type}\".format(filter_type=type(filter)))\n",
    "\n",
    "        return (where, params)\n",
    "\n",
    "    def search_query(\n",
    "            self, \n",
    "            query_embedding: Optional[Union[List[float], np.ndarray]], \n",
    "            limit: int = 10, \n",
    "            filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None, \n",
    "            predicates: Optional[Predicates] = None,\n",
    "            uuid_time_filter: Optional[UUIDTimeRange] = None,\n",
    "            ) -> Tuple[str, List]:\n",
    "        \"\"\"\n",
    "        Generates a similarity query.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, List]: A tuple containing the query and parameters.\n",
    "        \"\"\"\n",
    "        params: List[Any] = []\n",
    "        if query_embedding is not None:\n",
    "            distance = \"embedding {op} ${index}\".format(\n",
    "                op=self.distance_type, index=len(params)+1)\n",
    "            params = params + [query_embedding]\n",
    "            order_by_clause = \"ORDER BY {distance} ASC\".format(\n",
    "                distance=distance)\n",
    "        else:\n",
    "            distance = \"-1.0\"\n",
    "            order_by_clause = \"\"\n",
    "\n",
    "        if self.infer_filters:\n",
    "            if uuid_time_filter is None and isinstance(filter, dict):\n",
    "                if \"__start_date\" in filter or \"__end_date\" in filter:\n",
    "                    start_date = UUIDTimeRange._parse_datetime(filter.get(\"__start_date\"))\n",
    "                    end_date = UUIDTimeRange._parse_datetime(filter.get(\"__end_date\"))\n",
    "                    \n",
    "                    uuid_time_filter = UUIDTimeRange(start_date, end_date)\n",
    "                    \n",
    "                    if start_date is not None:\n",
    "                        del filter[\"__start_date\"]\n",
    "                    if end_date is not None:\n",
    "                        del filter[\"__end_date\"]\n",
    "\n",
    "\n",
    "        where_clauses = []\n",
    "        if filter is not None:\n",
    "            (where_filter, params) = self._where_clause_for_filter(params, filter)\n",
    "            where_clauses.append(where_filter)\n",
    "\n",
    "        if predicates is not None:\n",
    "            (where_predicates, params) = predicates.build_query(params)\n",
    "            where_clauses.append(where_predicates)\n",
    "\n",
    "        if uuid_time_filter is not None:\n",
    "            #if self.time_partition_interval is None:\n",
    "                #raise ValueError(\"\"\"uuid_time_filter is only supported when time_partitioning is enabled.\"\"\")\n",
    "            \n",
    "            (where_time, params) = uuid_time_filter.build_query(params)\n",
    "            where_clauses.append(where_time)\n",
    "        \n",
    "        if len(where_clauses) > 0:\n",
    "            where = \" AND \".join(where_clauses)\n",
    "        else:\n",
    "            where = \"TRUE\"\n",
    "\n",
    "        query = '''\n",
    "        SELECT\n",
    "            id, metadata, contents, embedding, {distance} as distance\n",
    "        FROM\n",
    "           {table_name}\n",
    "        WHERE \n",
    "           {where}\n",
    "        {order_by_clause}\n",
    "        LIMIT {limit}\n",
    "        '''.format(distance=distance, order_by_clause=order_by_clause, where=where, table_name=self._quote_ident(self.table_name), limit=limit)\n",
    "        return (query, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L475){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QueryBuilder.get_create_query\n",
       "\n",
       ">      QueryBuilder.get_create_query ()\n",
       "\n",
       "Generates a query to create the tables, indexes, and extensions needed to store the vector data."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L475){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QueryBuilder.get_create_query\n",
       "\n",
       ">      QueryBuilder.get_create_query ()\n",
       "\n",
       "Generates a query to create the tables, indexes, and extensions needed to store the vector data."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(QueryBuilder.get_create_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Async(QueryBuilder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            service_url: str,\n",
    "            table_name: str,\n",
    "            num_dimensions: int,\n",
    "            distance_type: str = 'cosine',\n",
    "            id_type='UUID',\n",
    "            time_partition_interval: Optional[timedelta] = None,\n",
    "            max_db_connections: Optional[int] = None,\n",
    "            infer_filters: bool = True,\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a async client for storing vector data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        service_url\n",
    "            The connection string for the database.\n",
    "        table_name\n",
    "            The name of the table.\n",
    "        num_dimensions\n",
    "            The number of dimensions for the embedding vector.\n",
    "        distance_type\n",
    "            The distance type for indexing.\n",
    "        id_type\n",
    "            The type of the id column. Can be either 'UUID' or 'TEXT'.\n",
    "        \"\"\"\n",
    "        self.builder = QueryBuilder(\n",
    "            table_name, num_dimensions, distance_type, id_type, time_partition_interval, infer_filters)\n",
    "        self.service_url = service_url\n",
    "        self.pool = None\n",
    "        self.max_db_connections = max_db_connections\n",
    "        self.time_partition_interval = time_partition_interval\n",
    "\n",
    "    async def _default_max_db_connections(self) -> int:\n",
    "        \"\"\"\n",
    "        Gets a default value for the number of max db connections to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.default_max_db_connection_query()\n",
    "        conn = await asyncpg.connect(dsn=self.service_url)\n",
    "        num_connections = await conn.fetchval(query)\n",
    "        await conn.close()\n",
    "        return num_connections\n",
    "\n",
    "\n",
    "    async def connect(self):\n",
    "        \"\"\"\n",
    "        Establishes a connection to a PostgreSQL database using asyncpg.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            asyncpg.Connection: The established database connection.\n",
    "        \"\"\"\n",
    "        if self.pool == None:\n",
    "            if self.max_db_connections == None:\n",
    "                self.max_db_connections = await self._default_max_db_connections()\n",
    "            async def init(conn):\n",
    "                await register_vector(conn)\n",
    "                # decode to a dict, but accept a string as input in upsert\n",
    "                await conn.set_type_codec(\n",
    "                    'jsonb',\n",
    "                    encoder=str,\n",
    "                    decoder=json.loads,\n",
    "                    schema='pg_catalog')\n",
    "\n",
    "            self.pool = await asyncpg.create_pool(dsn=self.service_url, init=init, min_size=1, max_size=self.max_db_connections)\n",
    "        return self.pool.acquire()\n",
    "\n",
    "    async def close(self):\n",
    "        if self.pool != None:\n",
    "            await self.pool.close()\n",
    "\n",
    "    async def table_is_empty(self):\n",
    "        \"\"\"\n",
    "        Checks if the table is empty.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            bool: True if the table is empty, False otherwise.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_row_exists_query()\n",
    "        async with await self.connect() as pool:\n",
    "            rec = await pool.fetchrow(query)\n",
    "            return rec == None\n",
    "\n",
    "    def munge_record(self, records) -> Iterable[Tuple[uuid.UUID, str, str, List[float]]]:\n",
    "        metadata_is_dict = isinstance(records[0][1], dict)\n",
    "        if metadata_is_dict:\n",
    "           records = map(lambda item: Async._convert_record_meta_to_json(item), records)\n",
    "\n",
    "        return records \n",
    "\n",
    "    def _convert_record_meta_to_json(item):\n",
    "        if not isinstance(item[1], dict):\n",
    "            raise ValueError(\n",
    "                \"Cannot mix dictionary and string metadata fields in the same upsert\")\n",
    "        return (item[0], json.dumps(item[1]), item[2], item[3])\n",
    "\n",
    "    async def upsert(self, records):\n",
    "        \"\"\"\n",
    "        Performs upsert operation for multiple records.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        records\n",
    "            List of records to upsert. Each record is a tuple of the form (id, metadata, contents, embedding).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        records = self.munge_record(records)\n",
    "        query = self.builder.get_upsert_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.executemany(query, records)\n",
    "\n",
    "    async def create_tables(self):\n",
    "        \"\"\"\n",
    "        Creates necessary tables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.get_create_query()\n",
    "        #don't use a connection pool for this because the vector extension may not be installed yet and if it's not installed, register_vector will fail.\n",
    "        conn = await asyncpg.connect(dsn=self.service_url)\n",
    "        await conn.execute(query)\n",
    "        await conn.close()\n",
    "\n",
    "    async def delete_all(self, drop_index=True):\n",
    "        \"\"\"\n",
    "        Deletes all data. Also drops the index if `drop_index` is true.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        if drop_index:\n",
    "            await self.drop_embedding_index()\n",
    "        query = self.builder.delete_all_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def delete_by_ids(self, ids: Union[List[uuid.UUID], List[str]]):\n",
    "        \"\"\"\n",
    "        Delete records by id.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_ids_query(ids)\n",
    "        async with await self.connect() as pool:\n",
    "            return await pool.fetch(query, *params)\n",
    "\n",
    "    async def delete_by_metadata(self, filter: Union[Dict[str, str], List[Dict[str, str]]]):\n",
    "        \"\"\"\n",
    "        Delete records by metadata filters.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_metadata_query(filter)\n",
    "        async with await self.connect() as pool:\n",
    "            return await pool.fetch(query, *params)\n",
    "\n",
    "    async def drop_table(self):\n",
    "        \"\"\"\n",
    "        Drops the table\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_table_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def _get_approx_count(self):\n",
    "        \"\"\"\n",
    "        Retrieves an approximate count of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: Approximate count of records.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_approx_count_query()\n",
    "        async with await self.connect() as pool:\n",
    "            rec = await pool.fetchrow(query)\n",
    "            return rec[0]\n",
    "\n",
    "    async def drop_embedding_index(self):\n",
    "        \"\"\"\n",
    "        Drop any index on the emedding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_embedding_index_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def create_embedding_index(self, index: BaseIndex):\n",
    "        \"\"\"\n",
    "        Creates an index for the table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "            The index to create.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            None\n",
    "        \"\"\"\n",
    "        #todo: can we make geting the records lazy?\n",
    "        num_records = await self._get_approx_count()\n",
    "        query = self.builder.create_embedding_index_query(index, lambda: num_records)\n",
    "        \n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def search(self,\n",
    "                     query_embedding: Optional[List[float]] = None, \n",
    "                     limit: int = 10,\n",
    "                     filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n",
    "                     predicates: Optional[Predicates] = None,\n",
    "                     uuid_time_filter: Optional[UUIDTimeRange] = None,\n",
    "                     ): \n",
    "        \"\"\"\n",
    "        Retrieves similar records using a similarity query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_embedding \n",
    "            The query embedding vector.\n",
    "        limit \n",
    "            The number of nearest neighbors to retrieve.\n",
    "        filter \n",
    "            A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched).\n",
    "        predicates\n",
    "            A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, |, and ~).\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            List: List of similar records.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.search_query(\n",
    "            query_embedding, limit, filter, predicates, uuid_time_filter)\n",
    "        async with await self.connect() as pool:\n",
    "            return await pool.fetch(query, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Async.create_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L843){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Async.create_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L944){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.search\n",
       "\n",
       ">      Async.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                    filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=No\n",
       ">                    ne, predicates:Optional[__main__.Predicates]=None,\n",
       ">                    uuid_time_filter:Optional[__main__.UUIDTimeRange]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| uuid_time_filter | Optional | None |  |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L944){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.search\n",
       "\n",
       ">      Async.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                    filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=No\n",
       ">                    ne, predicates:Optional[__main__.Predicates]=None,\n",
       ">                    uuid_time_filter:Optional[__main__.UUIDTimeRange]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| uuid_time_filter | Optional | None |  |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Async.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "con = await asyncpg.connect(service_url)\n",
    "await con.execute(\"DROP TABLE IF EXISTS data_table;\")\n",
    "await con.execute(\"DROP EXTENSION IF EXISTS vector CASCADE;\")\n",
    "await con.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Async(service_url, \"data_table\", 2)\n",
    "await vec.create_tables()\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "await vec.upsert([(uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = await vec.table_is_empty()\n",
    "assert not empty\n",
    "\n",
    "await vec.upsert([\n",
    "    (uuid.uuid4(), '''{\"key\":\"val\"}''', \"the brown fox\", [1.0, 1.3]),\n",
    "    (uuid.uuid4(), '''{\"key\":\"val2\", \"key_10\": \"10\", \"key_11\": \"11.3\"}''', \"the brown fox\", [1.0, 1.4]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.5]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val2\"}''', \"the brown fox\", [1.0, 1.7]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.9]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 100.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 101.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key_1\":\"val_1\", \"key_2\":\"val_2\"}''',\n",
    "     \"the brown fox\", [1.0, 1.8]),\n",
    "])\n",
    "\n",
    "await vec.create_embedding_index(IvfflatIndex())\n",
    "await vec.drop_embedding_index()\n",
    "await vec.create_embedding_index(IvfflatIndex(100))\n",
    "await vec.drop_embedding_index()\n",
    "await vec.create_embedding_index(HNSWIndex())\n",
    "await vec.drop_embedding_index()\n",
    "await vec.create_embedding_index(HNSWIndex(20, 125))\n",
    "await vec.drop_embedding_index()\n",
    "await vec.create_embedding_index(TimescaleVectorIndex())\n",
    "await vec.drop_embedding_index()\n",
    "await vec.create_embedding_index(TimescaleVectorIndex(False, 50, 50, 1.5))\n",
    "\n",
    "rec = await vec.search([1.0, 2.0])\n",
    "assert len(rec) == 10\n",
    "rec = await vec.search([1.0, 2.0], limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = await vec.search(limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"val2\"})\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"does not exist\"})\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\"})\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search(limit=4, filter={\"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "rec = await vec.search(limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}, {\"no such key\": \"no such val\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "assert isinstance(rec[0][SEARCH_RESULT_METADATA_IDX], dict)\n",
    "assert isinstance(rec[0][\"metadata\"], dict)\n",
    "assert rec[0][\"contents\"] == \"the brown fox\"\n",
    "\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\")))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"==\", \"val2\")))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key\", \"==\", \"val2\"))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"<\", 100))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"<\", 10))\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"<=\", 10))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"<=\", 10.0))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_11\", \"<=\", 11.3))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search(limit=4, predicates=Predicates(\"key_11\", \">=\", 11.29999))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_11\", \"<\", 11.299999))\n",
    "assert len(rec) == 0\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*[(\"key\", \"val2\"), (\"key_10\", \"<\", 100)]))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\"), (\"key_10\", \"<\", 100), operator='AND'))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\"), (\"key_2\", \"val_2\"), operator='OR'))\n",
    "assert len(rec) == 2\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"<\", 100) & (Predicates(\"key\",\"==\", \"val2\",) | Predicates(\"key_2\", \"==\", \"val_2\"))) \n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key_10\", \"<\", 100) and (Predicates(\"key\",\"==\", \"val2\") or Predicates(\"key_2\",\"==\", \"val_2\"))) \n",
    "assert len(rec) == 1\n",
    "rec = await vec.search(limit=4, predicates=~Predicates((\"key\", \"val2\"), (\"key_10\", \"<\", 100)))\n",
    "assert len(rec) == 4\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except ValueError as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries opposite order\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "await vec.delete_by_ids([rec[0][\"id\"]])\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 1\n",
    "await vec.delete_by_metadata([{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "assert len(rec) == 4\n",
    "await vec.delete_by_metadata([{\"key2\": \"val\"}])\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "assert len(rec) == 0\n",
    "\n",
    "assert not await vec.table_is_empty()\n",
    "await vec.delete_all()\n",
    "assert await vec.table_is_empty()\n",
    "\n",
    "await vec.drop_table()\n",
    "await vec.close()\n",
    "\n",
    "vec = Async(service_url, \"data_table\", 2, id_type=\"TEXT\")\n",
    "await vec.create_tables()\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "await vec.upsert([(\"Not a valid UUID\", {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = await vec.table_is_empty()\n",
    "assert not empty\n",
    "await vec.delete_by_ids([\"Not a valid UUID\"])\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "await vec.drop_table()\n",
    "await vec.close()\n",
    "\n",
    "vec = Async(service_url, \"data_table\", 2, time_partition_interval=timedelta(seconds=60))\n",
    "await vec.create_tables()\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "id = uuid.uuid1()\n",
    "await vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = await vec.table_is_empty()\n",
    "assert not empty\n",
    "await vec.delete_by_ids([id])\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert with uuid type 4 in time partitioned table\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "specific_datetime = datetime(2018, 8, 10, 15, 30, 0)\n",
    "await vec.upsert([\n",
    "        # current time\n",
    "        (uuid.uuid1(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n",
    "        #time in 2018\n",
    "        (uuid_from_time(specific_datetime),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "])\n",
    "assert not await vec.table_is_empty()\n",
    "\n",
    "#check all the possible ways to specify a date range\n",
    "async def search_date(start_date, end_date, expected):\n",
    "    #using uuid_time_filter\n",
    "    rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date, end_date))\n",
    "    assert len(rec) == expected\n",
    "    rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(str(start_date), str(end_date)))\n",
    "    assert len(rec) == expected\n",
    "    \n",
    "    #using filters\n",
    "    filter = {}\n",
    "    if start_date is not None:\n",
    "        filter[\"__start_date\"] = start_date\n",
    "    if end_date is not None:\n",
    "        filter[\"__end_date\"] = end_date\n",
    "    rec = await vec.search([1.0, 2.0], limit=4, filter=filter)\n",
    "    assert len(rec) == expected\n",
    "    #using filters with string dates\n",
    "    filter = {}\n",
    "    if start_date is not None:\n",
    "        filter[\"__start_date\"] = str(start_date)\n",
    "    if end_date is not None:\n",
    "        filter[\"__end_date\"] = str(end_date)\n",
    "    rec = await vec.search([1.0, 2.0], limit=4, filter=filter)\n",
    "    assert len(rec) == expected\n",
    "    #using predicates\n",
    "    predicates = []\n",
    "    if start_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \">=\", start_date))\n",
    "    if end_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \"<\", end_date))\n",
    "    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n",
    "    assert len(rec) == expected\n",
    "    #using predicates with string dates\n",
    "    predicates = []\n",
    "    if start_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \">=\", str(start_date)))\n",
    "    if end_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \"<\", str(end_date)))\n",
    "    rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n",
    "    assert len(rec) == expected\n",
    "\n",
    "await search_date(specific_datetime-timedelta(days=7), specific_datetime+timedelta(days=7), 1)\n",
    "await search_date(specific_datetime-timedelta(days=7), None, 2)\n",
    "await search_date(None, specific_datetime+timedelta(days=7), 1)\n",
    "await search_date(specific_datetime-timedelta(days=7), specific_datetime-timedelta(days=2), 0)\n",
    "\n",
    "#check timedelta handling\n",
    "rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date=specific_datetime, time_delta=timedelta(days=7)))\n",
    "assert len(rec) == 1\n",
    "#end is exclusive\n",
    "rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime, time_delta=timedelta(days=7)))\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime+timedelta(seconds=1), time_delta=timedelta(days=7)))\n",
    "assert len(rec) == 1\n",
    "await vec.drop_table()\n",
    "await vec.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import psycopg2.pool\n",
    "from contextlib import contextmanager\n",
    "import psycopg2.extras\n",
    "import pgvector.psycopg2\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sync:\n",
    "    translated_queries: Dict[str, str] = {}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            service_url: str,\n",
    "            table_name: str,\n",
    "            num_dimensions: int,\n",
    "            distance_type: str = 'cosine',\n",
    "            id_type='UUID',\n",
    "            time_partition_interval: Optional[timedelta] = None,\n",
    "            max_db_connections: Optional[int] = None,\n",
    "            infer_filters: bool = True,\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a sync client for storing vector data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        service_url\n",
    "            The connection string for the database.\n",
    "        table_name\n",
    "            The name of the table.\n",
    "        num_dimensions\n",
    "            The number of dimensions for the embedding vector.\n",
    "        distance_type\n",
    "            The distance type for indexing.\n",
    "        id_type\n",
    "            The type of the primary id column. Can be either 'UUID' or 'TEXT'.\n",
    "        \"\"\"\n",
    "        self.builder = QueryBuilder(\n",
    "            table_name, num_dimensions, distance_type, id_type, time_partition_interval, infer_filters)\n",
    "        self.service_url = service_url\n",
    "        self.pool = None\n",
    "        self.max_db_connections = max_db_connections\n",
    "        self.time_partition_interval = time_partition_interval\n",
    "        psycopg2.extras.register_uuid()\n",
    "\n",
    "    def default_max_db_connections(self):\n",
    "        \"\"\"\n",
    "        Gets a default value for the number of max db connections to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.default_max_db_connection_query()\n",
    "        conn = psycopg2.connect(dsn=self.service_url)\n",
    "        with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                num_connections = cur.fetchone() \n",
    "        conn.close()\n",
    "        return num_connections[0]\n",
    "\n",
    "    @contextmanager\n",
    "    def connect(self):\n",
    "        \"\"\"\n",
    "        Establishes a connection to a PostgreSQL database using psycopg2 and allows it's\n",
    "        use in a context manager.\n",
    "        \"\"\"\n",
    "        if self.pool == None:\n",
    "            if self.max_db_connections == None:\n",
    "                self.max_db_connections = self.default_max_db_connections()\n",
    "\n",
    "            self.pool = psycopg2.pool.SimpleConnectionPool(\n",
    "                1, self.max_db_connections, dsn=self.service_url, cursor_factory=psycopg2.extras.DictCursor)\n",
    "\n",
    "        connection = self.pool.getconn()\n",
    "        pgvector.psycopg2.register_vector(connection)\n",
    "        try:\n",
    "            yield connection\n",
    "            connection.commit()\n",
    "        finally:\n",
    "            self.pool.putconn(connection)\n",
    "\n",
    "    def close(self):\n",
    "        if self.pool != None:\n",
    "            self.pool.closeall()\n",
    "\n",
    "    def _translate_to_pyformat(self, query_string, params):\n",
    "        \"\"\"\n",
    "        Translates dollar sign number parameters and list parameters to pyformat strings.\n",
    "\n",
    "        Args:\n",
    "            query_string (str): The query string with parameters.\n",
    "            params (list): List of parameter values.\n",
    "\n",
    "        Returns:\n",
    "            str: The query string with translated pyformat parameters.\n",
    "            dict: A dictionary mapping parameter numbers to their values.\n",
    "        \"\"\"\n",
    "\n",
    "        translated_params = {}\n",
    "        if params != None:\n",
    "            for idx, param in enumerate(params):\n",
    "                translated_params[str(idx+1)] = param\n",
    "\n",
    "        if query_string in self.translated_queries:\n",
    "            return self.translated_queries[query_string], translated_params\n",
    "\n",
    "        dollar_params = re.findall(r'\\$[0-9]+', query_string)\n",
    "        translated_string = query_string\n",
    "        for dollar_param in dollar_params:\n",
    "            # Extract the number after the $\n",
    "            param_number = int(dollar_param[1:])\n",
    "            if params != None:\n",
    "                pyformat_param = '%s' if param_number == 0 else f'%({param_number})s'\n",
    "            else:\n",
    "                pyformat_param = '%s'\n",
    "            translated_string = translated_string.replace(\n",
    "                dollar_param, pyformat_param)\n",
    "\n",
    "        self.translated_queries[query_string] = translated_string\n",
    "        return self.translated_queries[query_string], translated_params\n",
    "\n",
    "    def table_is_empty(self):\n",
    "        \"\"\"\n",
    "        Checks if the table is empty.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            bool: True if the table is empty, False otherwise.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_row_exists_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                rec = cur.fetchone()\n",
    "                return rec == None\n",
    "    \n",
    "    def munge_record(self, records) -> Iterable[Tuple[uuid.UUID, str, str, List[float]]]:\n",
    "        metadata_is_dict = isinstance(records[0][1], dict)\n",
    "        if metadata_is_dict:\n",
    "           records = map(lambda item: Sync._convert_record_meta_to_json(item), records)\n",
    "\n",
    "        return records\n",
    "\n",
    "\n",
    "    def _convert_record_meta_to_json(item):\n",
    "        if not isinstance(item[1], dict):\n",
    "            raise ValueError(\n",
    "                \"Cannot mix dictionary and string metadata fields in the same upsert\")\n",
    "        return (item[0], json.dumps(item[1]), item[2], item[3])\n",
    "\n",
    "    def upsert(self, records):\n",
    "        \"\"\"\n",
    "        Performs upsert operation for multiple records.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        records\n",
    "            Records to upsert.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        records = self.munge_record(records)\n",
    "        query = self.builder.get_upsert_query()\n",
    "        query, _ = self._translate_to_pyformat(query, None)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.executemany(query, records)\n",
    "\n",
    "    def create_tables(self):\n",
    "        \"\"\"\n",
    "        Creates necessary tables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.get_create_query()\n",
    "        #don't use a connection pool for this because the vector extension may not be installed yet and if it's not installed, register_vector will fail.\n",
    "        conn = psycopg2.connect(dsn=self.service_url)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def delete_all(self, drop_index=True):\n",
    "        \"\"\"\n",
    "        Deletes all data. Also drops the index if `drop_index` is true.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        if drop_index:\n",
    "            self.drop_embedding_index()\n",
    "        query = self.builder.delete_all_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def delete_by_ids(self, ids: Union[List[uuid.UUID], List[str]]):\n",
    "        \"\"\"\n",
    "        Delete records by id.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ids\n",
    "            List of ids to delete.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_ids_query(ids)\n",
    "        query, params = self._translate_to_pyformat(query, params)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query, params)\n",
    "\n",
    "    def delete_by_metadata(self, filter: Union[Dict[str, str], List[Dict[str, str]]]):\n",
    "        \"\"\"\n",
    "        Delete records by metadata filters.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_metadata_query(filter)\n",
    "        query, params = self._translate_to_pyformat(query, params)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query, params)\n",
    "\n",
    "    def drop_table(self):\n",
    "        \"\"\"\n",
    "        Drops the table\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_table_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def _get_approx_count(self):\n",
    "        \"\"\"\n",
    "        Retrieves an approximate count of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: Approximate count of records.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_approx_count_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                rec = cur.fetchone()\n",
    "                return rec[0]\n",
    "\n",
    "    def drop_embedding_index(self):\n",
    "        \"\"\"\n",
    "        Drop any index on the emedding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_embedding_index_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "    \n",
    "    def create_embedding_index(self, index: BaseIndex):\n",
    "        \"\"\"\n",
    "        Creates an index on the embedding for the table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "            The index to create.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.create_embedding_index_query(index, lambda: self._get_approx_count())    \n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def search(self, \n",
    "    query_embedding: Optional[List[float]] = None, \n",
    "    limit: int = 10, \n",
    "    filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n",
    "    predicates: Optional[Predicates] = None,\n",
    "    uuid_time_filter: Optional[UUIDTimeRange] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Retrieves similar records using a similarity query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_embedding \n",
    "            The query embedding vector.\n",
    "        limit \n",
    "            The number of nearest neighbors to retrieve.\n",
    "        filter \n",
    "            A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched).\n",
    "        predicates\n",
    "            A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, |, and ~).\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            List: List of similar records.\n",
    "        \"\"\"\n",
    "        if query_embedding is not None:\n",
    "            query_embedding_np = np.array(query_embedding)\n",
    "        else:\n",
    "            query_embedding_np = None\n",
    "\n",
    "        (query, params) = self.builder.search_query(\n",
    "            query_embedding_np, limit, filter, predicates, uuid_time_filter)\n",
    "        query, params = self._translate_to_pyformat(query, params)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query, params)\n",
    "                return cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L1147){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.create_tables\n",
       "\n",
       ">      Sync.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L1147){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.create_tables\n",
       "\n",
       ">      Sync.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sync.create_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L1127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.upsert\n",
       "\n",
       ">      Sync.upsert (records)\n",
       "\n",
       "Performs upsert operation for multiple records.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| records |  | Records to upsert. |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L1127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.upsert\n",
       "\n",
       ">      Sync.upsert (records)\n",
       "\n",
       "Performs upsert operation for multiple records.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| records |  | Records to upsert. |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sync.upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L1262){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.search\n",
       "\n",
       ">      Sync.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                   filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=Non\n",
       ">                   e, predicates:Optional[__main__.Predicates]=None,\n",
       ">                   uuid_time_filter:Optional[__main__.UUIDTimeRange]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| uuid_time_filter | Optional | None |  |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L1262){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.search\n",
       "\n",
       ">      Sync.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                   filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=Non\n",
       ">                   e, predicates:Optional[__main__.Predicates]=None,\n",
       ">                   uuid_time_filter:Optional[__main__.UUIDTimeRange]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| uuid_time_filter | Optional | None |  |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sync.search)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "con = await asyncpg.connect(service_url)\n",
    "await con.execute(\"DROP TABLE IF EXISTS data_table;\")\n",
    "await con.execute(\"DROP EXTENSION IF EXISTS vector CASCADE\")\n",
    "await con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Sync(service_url, \"data_table\", 2)\n",
    "vec.create_tables()\n",
    "empty = vec.table_is_empty()\n",
    "\n",
    "assert empty\n",
    "vec.upsert([(uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = vec.table_is_empty()\n",
    "assert not empty\n",
    "\n",
    "vec.upsert([\n",
    "    (uuid.uuid4(), '''{\"key\":\"val\"}''', \"the brown fox\", [1.0, 1.3]),\n",
    "    (uuid.uuid4(), '''{\"key\":\"val2\"}''', \"the brown fox\", [1.0, 1.4]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.5]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val2\"}''', \"the brown fox\", [1.0, 1.7]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.9]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 100.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 101.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key_1\":\"val_1\", \"key_2\":\"val_2\"}''',\n",
    "     \"the brown fox\", [1.0, 1.8]),\n",
    "])\n",
    "\n",
    "vec.create_embedding_index(IvfflatIndex())\n",
    "vec.drop_embedding_index()\n",
    "vec.create_embedding_index(IvfflatIndex(100))\n",
    "vec.drop_embedding_index()\n",
    "vec.create_embedding_index(HNSWIndex())\n",
    "vec.drop_embedding_index()\n",
    "vec.create_embedding_index(HNSWIndex(20, 125))\n",
    "vec.drop_embedding_index()\n",
    "vec.create_embedding_index(TimescaleVectorIndex())\n",
    "vec.drop_embedding_index()\n",
    "vec.create_embedding_index(TimescaleVectorIndex(False, 50, 50, 1.5))\n",
    "\n",
    "rec = vec.search([1.0, 2.0])\n",
    "assert len(rec) == 10\n",
    "rec = vec.search(np.array([1.0, 2.0]))\n",
    "assert len(rec) == 10\n",
    "rec = vec.search([1.0, 2.0], limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = vec.search(limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"val2\"})\n",
    "assert len(rec) == 1\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"does not exist\"})\n",
    "assert len(rec) == 0\n",
    "rec = vec.search(limit=4, filter={\"key2\": \"does not exist\"})\n",
    "assert len(rec) == 0\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\"})\n",
    "assert len(rec) == 1\n",
    "rec = vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n",
    "assert len(rec) == 1\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\n",
    "                 \"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n",
    "assert len(rec) == 0\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\n",
    "                 \"key2\": \"val2\"}, {\"no such key\": \"no such val\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except ValueError as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries opposite order\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "rec = vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n",
    "assert rec[0][SEARCH_RESULT_CONTENTS_IDX] == 'the brown fox'\n",
    "assert rec[0][\"contents\"] == 'the brown fox'\n",
    "assert rec[0][SEARCH_RESULT_METADATA_IDX] == {\n",
    "    'key_1': 'val_1', 'key_2': 'val_2'}\n",
    "assert rec[0][\"metadata\"] == {\n",
    "    'key_1': 'val_1', 'key_2': 'val_2'}\n",
    "assert isinstance(rec[0][SEARCH_RESULT_METADATA_IDX], dict)\n",
    "assert rec[0][SEARCH_RESULT_DISTANCE_IDX] == 0.0009438353921149556\n",
    "assert rec[0][\"distance\"] == 0.0009438353921149556\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates(\"key\",\"==\", \"val2\"))\n",
    "assert len(rec) == 1\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "len(rec) == 2\n",
    "vec.delete_by_ids([rec[0][SEARCH_RESULT_ID_IDX]])\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 1\n",
    "vec.delete_by_metadata([{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 0\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "assert len(rec) == 4\n",
    "vec.delete_by_metadata([{\"key2\": \"val\"}])\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "len(rec) == 0\n",
    "\n",
    "assert not vec.table_is_empty()\n",
    "vec.delete_all()\n",
    "assert vec.table_is_empty()\n",
    "\n",
    "vec.drop_table()\n",
    "vec.close()\n",
    "\n",
    "vec = Sync(service_url, \"data_table\", 2, id_type=\"TEXT\")\n",
    "vec.create_tables()\n",
    "assert vec.table_is_empty()\n",
    "vec.upsert([(\"Not a valid UUID\", {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "assert not vec.table_is_empty()\n",
    "vec.delete_by_ids([\"Not a valid UUID\"])\n",
    "assert vec.table_is_empty()\n",
    "vec.drop_table()\n",
    "vec.close()\n",
    "\n",
    "vec = Sync(service_url, \"data_table\", 2, time_partition_interval=timedelta(seconds=60))\n",
    "vec.create_tables()\n",
    "assert vec.table_is_empty()\n",
    "id = uuid.uuid1()\n",
    "vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "assert not vec.table_is_empty()\n",
    "vec.delete_by_ids([id])\n",
    "assert vec.table_is_empty()\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert with uuid type 4 in time partitioned table\n",
    "    vec.upsert([\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "    #pass\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "specific_datetime = datetime(2018, 8, 10, 15, 30, 0)\n",
    "vec.upsert([\n",
    "        # current time\n",
    "        (uuid.uuid1(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n",
    "        #time in 2018\n",
    "        (uuid_from_time(specific_datetime),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "])\n",
    "\n",
    "def search_date(start_date, end_date, expected):\n",
    "    #using uuid_time_filter\n",
    "    rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date, end_date))\n",
    "    assert len(rec) == expected\n",
    "    rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(str(start_date), str(end_date)))\n",
    "    assert len(rec) == expected\n",
    "    \n",
    "    #using filters\n",
    "    filter = {}\n",
    "    if start_date is not None:\n",
    "        filter[\"__start_date\"] = start_date\n",
    "    if end_date is not None:\n",
    "        filter[\"__end_date\"] = end_date\n",
    "    rec = vec.search([1.0, 2.0], limit=4, filter=filter)\n",
    "    assert len(rec) == expected\n",
    "    #using filters with string dates\n",
    "    filter = {}\n",
    "    if start_date is not None:\n",
    "        filter[\"__start_date\"] = str(start_date)\n",
    "    if end_date is not None:\n",
    "        filter[\"__end_date\"] = str(end_date)\n",
    "    rec = vec.search([1.0, 2.0], limit=4, filter=filter)\n",
    "    assert len(rec) == expected\n",
    "    #using predicates\n",
    "    predicates = []\n",
    "    if start_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \">=\", start_date))\n",
    "    if end_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \"<\", end_date))\n",
    "    rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n",
    "    assert len(rec) == expected\n",
    "    #using predicates with string dates\n",
    "    predicates = []\n",
    "    if start_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \">=\", str(start_date)))\n",
    "    if end_date is not None:\n",
    "        predicates.append((\"__uuid_timestamp\", \"<\", str(end_date)))\n",
    "    rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates(*predicates))\n",
    "    assert len(rec) == expected\n",
    "\n",
    "assert not vec.table_is_empty()\n",
    "\n",
    "search_date(specific_datetime-timedelta(days=7), specific_datetime+timedelta(days=7), 1)\n",
    "search_date(specific_datetime-timedelta(days=7), None, 2)\n",
    "search_date(None, specific_datetime+timedelta(days=7), 1)\n",
    "search_date(specific_datetime-timedelta(days=7), specific_datetime-timedelta(days=2), 0)\n",
    "\n",
    "#check timedelta handling\n",
    "rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(start_date=specific_datetime, time_delta=timedelta(days=7)))\n",
    "assert len(rec) == 1\n",
    "#end is exclusive\n",
    "rec =  vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime, time_delta=timedelta(days=7)))\n",
    "assert len(rec) == 0\n",
    "rec = vec.search([1.0, 2.0], limit=4, uuid_time_filter=UUIDTimeRange(end_date=specific_datetime+timedelta(seconds=1), time_delta=timedelta(days=7)))\n",
    "assert len(rec) == 1\n",
    "vec.drop_table()\n",
    "vec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
